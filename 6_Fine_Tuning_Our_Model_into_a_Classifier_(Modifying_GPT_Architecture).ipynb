{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Hello everyone! So far on this journey, we’ve managed to build our very own version of GPT-2, and later brought in the open-source weights provided by OpenAI. We walked through the idea of pretraining—imagining what it would take to train on massive datasets for countless epochs with enormous resources—before grounding ourselves with the already available pretrained weights. Now we are going to fine-tune the model to work as an email classifier, giving us our first real taste of a specific task."
      ],
      "metadata": {
        "id": "UbJoCAxbnI7k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eR5BkRPYKS4w",
        "outputId": "c24bcb09-e5ef-4ba1-a933-dfe277ff4e4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sms_spam_collection/SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import ssl\n",
        "import zipfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "zip_path = \"sms_spam_collection.zip\"\n",
        "extracted_path = \"sms_spam_collection\"\n",
        "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
        "\n",
        "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
        "    if data_file_path.exists():\n",
        "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
        "        return\n",
        "\n",
        "    # Create an unverified SSL context\n",
        "    ssl_context = ssl._create_unverified_context()\n",
        "\n",
        "    # Downloading the file\n",
        "    with urllib.request.urlopen(url, context=ssl_context) as response:\n",
        "        with open(zip_path, \"wb\") as out_file:\n",
        "            out_file.write(response.read())\n",
        "\n",
        "    # Unzipping the file\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extracted_path)\n",
        "\n",
        "    # Add .tsv file extension\n",
        "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
        "    os.rename(original_file_path, data_file_path)\n",
        "    print(f\"File downloaded and saved as {data_file_path}\")\n",
        "\n",
        "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "YJ-atZ__KlFa",
        "outputId": "7bc3bd56-6df0-4613-a587-3fa2357735a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Label                                               Text\n",
              "0      ham  Go until jurong point, crazy.. Available only ...\n",
              "1      ham                      Ok lar... Joking wif u oni...\n",
              "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      ham  U dun say so early hor... U c already then say...\n",
              "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
              "...    ...                                                ...\n",
              "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
              "5568   ham               Will ü b going to esplanade fr home?\n",
              "5569   ham  Pity, * was in mood for that. So...any other s...\n",
              "5570   ham  The guy did some bitching but I acted like i'd...\n",
              "5571   ham                         Rofl. Its true to its name\n",
              "\n",
              "[5572 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d201f8ae-086e-4978-9f67-51dbacd78898\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>spam</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>ham</td>\n",
              "      <td>Will ü b going to esplanade fr home?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>ham</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>ham</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>ham</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d201f8ae-086e-4978-9f67-51dbacd78898')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d201f8ae-086e-4978-9f67-51dbacd78898 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d201f8ae-086e-4978-9f67-51dbacd78898');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-41d94dcd-ea07-4c67-9fe1-f04fc1b0f7a1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-41d94dcd-ea07-4c67-9fe1-f04fc1b0f7a1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-41d94dcd-ea07-4c67-9fe1-f04fc1b0f7a1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_3f8a8d3a-944c-44c4-8f90-d5e946e843ee\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3f8a8d3a-944c-44c4-8f90-d5e946e843ee button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5572,\n  \"fields\": [\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"spam\",\n          \"ham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5169,\n        \"samples\": [\n          \"K, makes sense, btw carlos is being difficult so you guys are gonna smoke while I go pick up the second batch and get gas\",\n          \"URGENT! Your mobile No *********** WON a \\u00a32,000 Bonus Caller Prize on 02/06/03! This is the 2nd attempt to reach YOU! Call 09066362220 ASAP! BOX97N7QP, 150ppm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "HS_4olTbKpCR",
        "outputId": "c2a6f268-b96f-46cc-e598-3f33521cd421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Label\n",
              "ham     4825\n",
              "spam     747\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ham</th>\n",
              "      <td>4825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>spam</th>\n",
              "      <td>747</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImW6_clGKS4y",
        "outputId": "9b7fd7d1-661e-495c-b537-1539ab52dac9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label\n",
            "ham     747\n",
            "spam    747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "def create_balanced_dataset(df):\n",
        "\n",
        "    # Count the instances of \"spam\"\n",
        "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
        "\n",
        "    # Randomly sample \"ham\" instances to match the number of \"spam\" instances\n",
        "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
        "\n",
        "    # Combine ham \"subset\" with \"spam\"\n",
        "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
        "\n",
        "    return balanced_df\n",
        "\n",
        "balanced_df = create_balanced_dataset(df)\n",
        "print(balanced_df[\"Label\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
      ],
      "metadata": {
        "id": "5aTeHffrK1mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "jY9EfbnqK5Gw",
        "outputId": "c7ded81a-9032-4216-bfc5-98a73b38d241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Label                                               Text\n",
              "4307      0  Awww dat is sweet! We can think of something t...\n",
              "4138      0                             Just got to  &lt;#&gt;\n",
              "4831      0  The word \"Checkmate\" in chess comes from the P...\n",
              "4461      0  This is wishing you a great day. Moji told me ...\n",
              "5440      0      Thank you. do you generally date the brothas?\n",
              "...     ...                                                ...\n",
              "5537      1  Want explicit SEX in 30 secs? Ring 02073162414...\n",
              "5540      1  ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...\n",
              "5547      1  Had your contract mobile 11 Mnths? Latest Moto...\n",
              "5566      1  REMINDER FROM O2: To get 2.50 pounds free call...\n",
              "5567      1  This is the 2nd time we have tried 2 contact u...\n",
              "\n",
              "[1494 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-31e49a44-14ba-4ac0-93d0-5f57c3776fac\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4307</th>\n",
              "      <td>0</td>\n",
              "      <td>Awww dat is sweet! We can think of something t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4138</th>\n",
              "      <td>0</td>\n",
              "      <td>Just got to  &amp;lt;#&amp;gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4831</th>\n",
              "      <td>0</td>\n",
              "      <td>The word \"Checkmate\" in chess comes from the P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4461</th>\n",
              "      <td>0</td>\n",
              "      <td>This is wishing you a great day. Moji told me ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5440</th>\n",
              "      <td>0</td>\n",
              "      <td>Thank you. do you generally date the brothas?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5537</th>\n",
              "      <td>1</td>\n",
              "      <td>Want explicit SEX in 30 secs? Ring 02073162414...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5540</th>\n",
              "      <td>1</td>\n",
              "      <td>ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5547</th>\n",
              "      <td>1</td>\n",
              "      <td>Had your contract mobile 11 Mnths? Latest Moto...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5566</th>\n",
              "      <td>1</td>\n",
              "      <td>REMINDER FROM O2: To get 2.50 pounds free call...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>1</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1494 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-31e49a44-14ba-4ac0-93d0-5f57c3776fac')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-31e49a44-14ba-4ac0-93d0-5f57c3776fac button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-31e49a44-14ba-4ac0-93d0-5f57c3776fac');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f1cae2f1-7b97-448b-93ff-98369d5a0962\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f1cae2f1-7b97-448b-93ff-98369d5a0962')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f1cae2f1-7b97-448b-93ff-98369d5a0962 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_02526749-7bc2-482e-bd4a-561ca0a5f9b0\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('balanced_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_02526749-7bc2-482e-bd4a-561ca0a5f9b0 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('balanced_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "balanced_df",
              "summary": "{\n  \"name\": \"balanced_df\",\n  \"rows\": 1494,\n  \"fields\": [\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1388,\n        \"samples\": [\n          \"chile, please! It's only a  &lt;DECIMAL&gt;  hour drive for me. I come down all the time and will be subletting feb-april for audition season.\",\n          \"I only haf msn. It's yijue@hotmail.com\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXkx6GYcKS40"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "We create a random_split function to split the dataset into three parts: 70% for\n",
        "training, 10% for validation, and 20% for testing.\n",
        "\n",
        "(These ratios are common in machine\n",
        "learning to train, adjust, and evaluate models.)    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6IdC5hqKS40"
      },
      "outputs": [],
      "source": [
        "def random_split(df, train_frac, validation_frac):\n",
        "    # Shuffle the entire DataFrame\n",
        "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "\n",
        "    # Calculate split indices\n",
        "    train_end = int(len(df) * train_frac)\n",
        "    validation_end = train_end + int(len(df) * validation_frac)\n",
        "\n",
        "    # Split the DataFrame\n",
        "    train_df = df[:train_end]\n",
        "    validation_df = df[train_end:validation_end]\n",
        "    test_df = df[validation_end:]\n",
        "\n",
        "    return train_df, validation_df, test_df\n",
        "\n",
        "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
        "# Test size is implied to be 0.2 as the remainder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w00h-QXaKS40",
        "outputId": "0f7fdf1e-b894-4363-8917-700b638cb3db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1045\n",
            "149\n",
            "300\n"
          ]
        }
      ],
      "source": [
        "print(len(train_df))\n",
        "print(len(validation_df))\n",
        "print(len(test_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5UxwYS6KS41"
      },
      "outputs": [],
      "source": [
        "train_df.to_csv(\"train.csv\", index=None)\n",
        "validation_df.to_csv(\"validation.csv\", index=None)\n",
        "test_df.to_csv(\"test.csv\", index=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD4pBaG4KS42"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Previously, we utilized a sliding window technique to generate uniformly\n",
        "sized text chunks, which were then grouped into batches for more efficient model training.\n",
        "Each chunk functioned as an individual training instance\n",
        "\n",
        "In the case of email spam classification, have two primary options:\n",
        "\n",
        "(1) Truncate all messages to the length of the shortest message in the\n",
        "dataset or batch.\n",
        "\n",
        "(2) Pad all messages to the length of the longest message in the dataset or\n",
        "batch.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr9jBNhOKS42"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "Option 1 is computationally cheaper, but it may result in significant information loss if\n",
        "shorter messages are much smaller than the average or longest messages, potentially\n",
        "reducing model performance.\n",
        "\n",
        "So, we opt for the second option, which preserves the entire\n",
        "content of all messages.\n",
        "\n",
        "To implement option 2, where all messages are padded to the length of the longest\n",
        "message in the dataset, we add padding tokens to all shorter messages.\n",
        "\n",
        "For this purpose,\n",
        "we use \"<|endoftext|>\" as a padding token, as discussed in chapter 2.\n",
        "\n",
        "    \n",
        "However, instead of appending the string \"<|endoftext|>\" to each of the text messages\n",
        "directly, we can add the token ID corresponding to \"<|endoftext|>\" to the encoded text\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wx8P9-TKS42"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "As we have seen earlier, we first need to implement a PyTorch Dataset, which\n",
        "specifies how the data is loaded and processed, before we can instantiate the data loaders.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku27Tw7LKS42"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "For this purpose, we define the SpamDataset class.\n",
        "\n",
        "This SpamDataset class handles several key tasks: it identifies the\n",
        "longest sequence in the training dataset, encodes the text messages, and ensures that all\n",
        "other sequences are padded with a padding token to match the length of the longest\n",
        "sequence.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fzjIuIaKS43"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "\n",
        "        # Pre-tokenize texts\n",
        "        self.encoded_texts = [\n",
        "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
        "        ]\n",
        "\n",
        "        if max_length is None:\n",
        "            self.max_length = self._longest_encoded_length()\n",
        "        else:\n",
        "            self.max_length = max_length\n",
        "\n",
        "            # Truncate sequences if they are longer than max_length\n",
        "            self.encoded_texts = [\n",
        "                encoded_text[:self.max_length]\n",
        "                for encoded_text in self.encoded_texts\n",
        "            ]\n",
        "\n",
        "        # Pad sequences to the longest sequence\n",
        "        self.encoded_texts = [\n",
        "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
        "            for encoded_text in self.encoded_texts\n",
        "        ]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        encoded = self.encoded_texts[index]\n",
        "        label = self.data.iloc[index][\"Label\"]\n",
        "        return (\n",
        "            torch.tensor(encoded, dtype=torch.long),\n",
        "            torch.tensor(label, dtype=torch.long)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _longest_encoded_length(self):\n",
        "        max_length = 0\n",
        "        for encoded_text in self.encoded_texts:\n",
        "            encoded_length = len(encoded_text)\n",
        "            if encoded_length > max_length:\n",
        "                max_length = encoded_length\n",
        "        return max_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0mvg1VTKS43"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "Step 1: Pre-tokenize texts\n",
        "    \n",
        "Step 2: Truncate sequences if they are longer than max_length\n",
        "    \n",
        "Step 3: Pad sequences to the longest sequence\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UOWunFGKS43"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "The SpamDataset class loads data from the CSV files we created earlier, tokenizes the text\n",
        "using the GPT-2 tokenizer from tiktoken and allows us to pad or truncate the sequences to\n",
        "a uniform length determined by either the longest sequence or a predefined maximum\n",
        "length.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Iwr_yCsKS43"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "This ensures each input tensor is of the same size, which is necessary to create the\n",
        "batches in the training data loader we implement next:\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoMcc5lnKS45",
        "outputId": "536c0dbf-6bb2-430a-dc10-7c53f0c96e27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "train_dataset = SpamDataset(\n",
        "    csv_file=\"train.csv\",\n",
        "    max_length=None,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(train_dataset.max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIcLFJ7gKS46"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "The code outputs 120, showing that the longest sequence contains no more than 120\n",
        "tokens, a common length for text messages.\n",
        "                       \n",
        "It's worth noting that the model can handle\n",
        "sequences of up to 1,024 tokens, given its context length limit.\n",
        "\n",
        "If your dataset includes\n",
        "longer texts, you can pass max_length=1024 when creating the training dataset in the\n",
        "preceding code to ensure that the data does not exceed the model's supported input\n",
        "(context) length.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy15xJGlKS46"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Next, we pad the validation and test sets to match the length of the longest training\n",
        "sequence.\n",
        "\n",
        "It's important to note that any validation and test set samples exceeding the\n",
        "length of the longest training example are truncated using\n",
        "encoded_text[:self.max_length] in the SpamDataset code we defined earlier.\n",
        "\n",
        "This\n",
        "truncation is optional; you could also set max_length=None for both validation and test\n",
        "sets, provided there are no sequences exceeding 1,024 tokens in these sets\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqmQ8UbFKS46",
        "outputId": "00134984-58fb-4e84-8eb9-767353797073",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120\n"
          ]
        }
      ],
      "source": [
        "val_dataset = SpamDataset(\n",
        "    csv_file=\"validation.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "test_dataset = SpamDataset(\n",
        "    csv_file=\"test.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(test_dataset.max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WnFOHU6KS47"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Using the datasets as inputs, we can instantiate the data loaders similarly to what we did earlier.\n",
        "\n",
        "However, in this case, the targets represent class labels rather than the next\n",
        "tokens in the text.\n",
        "\n",
        "For instance, choosing a batch size of 8, each batch will consist of 8\n",
        "training examples of length 120 and the corresponding class label of each example.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCfRaHuRKS47"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF-Nu3yEKS47"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "To ensure that the data loaders are working and are indeed returning batches of the\n",
        "expected size, we iterate over the training loader and then print the tensor dimensions of\n",
        "the last batch:\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp1ywlfHKS49",
        "outputId": "117d2896-e1a1-4b1f-e71e-d5473213c0b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "Input batch dimensions: torch.Size([8, 120])\n",
            "Label batch dimensions torch.Size([8])\n"
          ]
        }
      ],
      "source": [
        "print(\"Train loader:\")\n",
        "for input_batch, target_batch in train_loader:\n",
        "    pass\n",
        "\n",
        "print(\"Input batch dimensions:\", input_batch.shape)\n",
        "print(\"Label batch dimensions\", target_batch.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv1SE_YzKS4-"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "As we can see, the input batches consist of 8 training examples with 120 tokens each, as\n",
        "expected.\n",
        "\n",
        "The label tensor stores the class labels corresponding to the 8 training examples.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giTrKDY7KS4_"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Lastly, to get an idea of the dataset size, let's print the total number of batches in each\n",
        "dataset:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XttwCVccKS4_",
        "outputId": "74ba4d1d-a974-494c-8122-98532e02eb69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "130 training batches\n",
            "19 validation batches\n",
            "38 test batches\n"
          ]
        }
      ],
      "source": [
        "print(f\"{len(train_loader)} training batches\")\n",
        "print(f\"{len(val_loader)} validation batches\")\n",
        "print(f\"{len(test_loader)} test batches\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())"
      ],
      "metadata": {
        "id": "zcIzxRon_MnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "hzxAkzl08EP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    'vocab_size' : 50257,\n",
        "    'emb_dim' : 768,\n",
        "    'context_length' : 256,\n",
        "    'n_heads' : 12,\n",
        "    'n_layers' : 12,\n",
        "    'dropout_rate' : 0.1,\n",
        "    'qkv_bias' : False,\n",
        "    'model_name' : 'gpt-2'\n",
        "}"
      ],
      "metadata": {
        "id": "cBhdPpi08AUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))"
      ],
      "metadata": {
        "id": "-x5AgXcU8AUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "coqPWhoG8AUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift"
      ],
      "metadata": {
        "id": "gpXldXK58AUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "2gHupfu68AUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "      super().__init__() # d_in, d_out, context_length, dropout, num_heads, qkv_bias=False\n",
        "      self.attn = MultiHeadAttention(cfg['emb_dim'], cfg['emb_dim'], cfg['context_length'], cfg['dropout_rate'], cfg['n_heads'], cfg['qkv_bias'])\n",
        "      self.ffn = FeedForward(cfg)\n",
        "      self.norm1 = nn.LayerNorm(cfg['emb_dim'])\n",
        "      self.norm2 = nn.LayerNorm(cfg['emb_dim'])\n",
        "      self.dropout = nn.Dropout(cfg['dropout_rate'])\n",
        "\n",
        "  def forward(self, x):\n",
        "      shortcut = x\n",
        "      x = self.norm1(x)\n",
        "      x = self.attn(x)\n",
        "      x = self.dropout(x)\n",
        "      x = shortcut + x\n",
        "\n",
        "      shortcut = x\n",
        "      x = self.norm2(x)\n",
        "      x = self.ffn(x)\n",
        "      x = self.dropout(x)\n",
        "      x = shortcut + x\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "C9EmtHI28AUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "      super().__init__()\n",
        "      self.token_embedding = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
        "      self.positional_embedding = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
        "      self.dropout = nn.Dropout(cfg['dropout_rate'])\n",
        "\n",
        "      self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "      self.final_norm = nn.LayerNorm(cfg['emb_dim'])\n",
        "      self.lm_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "      batch_size, seq_len = x.shape\n",
        "      token_embeddings = self.token_embedding(x)\n",
        "      position_embeddings = self.positional_embedding(torch.arange(seq_len, device=x.device))\n",
        "      input_embeddings = token_embeddings + position_embeddings\n",
        "      x = self.dropout(input_embeddings)\n",
        "      x = self.trf_blocks(x)\n",
        "      x = self.final_norm(x)\n",
        "      x = self.lm_head(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "KdGl6iyL8AUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:].to(idx.device) # Explicitly move to device\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "J3c0Msgc8F7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bi-nmpFnK4Oh"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow>=2.15.0 tqdm>=4.66"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYvICBrsK4Oh",
        "outputId": "5a0dfad3-a0bf-481c-9ee1-59a93ae47978",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "tqdm version: 4.67.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"tqdm version:\", tqdm.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_download3 import download_and_load_gpt2"
      ],
      "metadata": {
        "id": "1_Dm4M2rjEkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR78jd4mkrRI",
        "outputId": "74026549-ef36-4818-b10a-a954670c0caa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/encoder.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/hparams.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Settings:\", settings)\n",
        "print(\"Parameter dictionary keys:\", params.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-28Xb-KwkuQR",
        "outputId": "22f261c7-3e1e-4fb6-c874-be21b4dffe1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
            "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Xo7pqZ3m9bZ",
        "outputId": "6c24d377-d3a6-4f97-bea9-3f0ab69f22ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'vocab_size': 50257,\n",
              " 'emb_dim': 768,\n",
              " 'context_length': 256,\n",
              " 'n_heads': 12,\n",
              " 'n_layers': 12,\n",
              " 'dropout_rate': 0.1,\n",
              " 'qkv_bias': False,\n",
              " 'model_name': 'gpt-2'}"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QpmkbL45nakh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsOAtSP67F23"
      },
      "outputs": [],
      "source": [
        "# Define model configurations in a dictionary for compactness\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "# Copy the base configuration and update with specific model settings\n",
        "model_name = \"gpt2-small (124M)\"  # Example model name\n",
        "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
        "NEW_CONFIG.update(model_configs[model_name])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})"
      ],
      "metadata": {
        "id": "xKgaMcvhDS-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NEW_CONFIG"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjyhIDDKDYCF",
        "outputId": "53370736-7b15-4324-cc3d-74ea750fadab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'vocab_size': 50257,\n",
              " 'emb_dim': 768,\n",
              " 'context_length': 1024,\n",
              " 'n_heads': 12,\n",
              " 'n_layers': 12,\n",
              " 'dropout_rate': 0.1,\n",
              " 'qkv_bias': True,\n",
              " 'model_name': 'gpt-2'}"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt = GPTModel(NEW_CONFIG)\n",
        "gpt.eval();"
      ],
      "metadata": {
        "id": "nAb0SZj5DXdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))"
      ],
      "metadata": {
        "id": "Iu2CXJwBDc25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.positional_embedding.weight = assign(gpt.positional_embedding.weight, params['wpe'])\n",
        "    gpt.token_embedding.weight = assign(gpt.token_embedding.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].attn.W_query.weight = assign(\n",
        "            gpt.trf_blocks[b].attn.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].attn.W_key.weight = assign(\n",
        "            gpt.trf_blocks[b].attn.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].attn.W_value.weight = assign(\n",
        "            gpt.trf_blocks[b].attn.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].attn.W_query.bias = assign(\n",
        "            gpt.trf_blocks[b].attn.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].attn.W_key.bias = assign(\n",
        "            gpt.trf_blocks[b].attn.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].attn.W_value.bias = assign(\n",
        "            gpt.trf_blocks[b].attn.W_value.bias, v_b)\n",
        "\n",
        "        gpt.trf_blocks[b].attn.out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].attn.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].attn.out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].attn.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].ffn.layers[0].weight = assign(\n",
        "            gpt.trf_blocks[b].ffn.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ffn.layers[0].bias = assign(\n",
        "            gpt.trf_blocks[b].ffn.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ffn.layers[2].weight = assign(\n",
        "            gpt.trf_blocks[b].ffn.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ffn.layers[2].bias = assign(\n",
        "            gpt.trf_blocks[b].ffn.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].norm1.weight = assign(\n",
        "            gpt.trf_blocks[b].norm1.weight,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm1.bias = assign(\n",
        "            gpt.trf_blocks[b].norm1.bias,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm2.weight = assign(\n",
        "            gpt.trf_blocks[b].norm2.weight,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm2.bias = assign(\n",
        "            gpt.trf_blocks[b].norm2.bias,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "\n",
        "    gpt.final_norm.weight = assign(gpt.final_norm.weight, params[\"g\"])\n",
        "    gpt.final_norm.bias = assign(gpt.final_norm.bias, params[\"b\"])\n",
        "    gpt.lm_head.weight = assign(gpt.lm_head.weight, params[\"wte\"])"
      ],
      "metadata": {
        "id": "ukX1TFcmD7zZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaf398c6",
        "outputId": "4b754fe6-6b06-4133-fd48-c04a393f8ce0"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device} device.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_weights_into_gpt(gpt, params)\n",
        "gpt.to(device);"
      ],
      "metadata": {
        "id": "3J-o2GMlEkWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "gpt.eval()\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "token_ids = generate(\n",
        "    model=gpt,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
        "    max_new_tokens=50,\n",
        "    context_size=NEW_CONFIG[\"context_length\"],\n",
        "    top_k=50,\n",
        "    temperature=1\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d82-L5vSEnkZ",
        "outputId": "89b8bbff-b7df-4ec3-9e49-d6ae09ceb567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you as far as the eye can see. (That's because you're only going to see it at those locations where you know you're seeing a pattern.) It's like riding your bike.\n",
            "\n",
            "If you know you're seeing that pattern — and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_2 = (\n",
        "    \"Answer with 'Spam' or 'Not Spam':\"\n",
        "    \" 'You are a winner you have been specially\"\n",
        "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
        ")\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "token_ids = generate(\n",
        "    model=gpt,\n",
        "    idx=text_to_token_ids(text_2, tokenizer).to(device),\n",
        "    max_new_tokens=23,\n",
        "    context_size=NEW_CONFIG[\"context_length\"],\n",
        "    top_k=50,\n",
        "    temperature=1\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcGKMhR_NJPK",
        "outputId": "d7fa6859-04f9-4446-8eca-ed0180a2dfaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Answer with 'Spam' or 'Not Spam': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
            "\n",
            "It's this last form of prize that's been so badly mistreated that two law firms in the city\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NEW_CONFIG"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmf3ZKoAOsKA",
        "outputId": "fcc13112-03e8-46e2-80dc-f4570fd4509e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'vocab_size': 50257,\n",
              " 'emb_dim': 768,\n",
              " 'context_length': 1024,\n",
              " 'n_heads': 12,\n",
              " 'n_layers': 12,\n",
              " 'dropout_rate': 0.1,\n",
              " 'qkv_bias': True,\n",
              " 'model_name': 'gpt-2'}"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSbmmA3CKS5I"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "To get the model ready for classification-finetuning, we first freeze the model, meaning that\n",
        "we make all layers non-trainable:\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YfFZXyxKS5I"
      },
      "outputs": [],
      "source": [
        "for param in gpt.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PjlPVVDKS5I"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Then, we replace the output layer (model.out_head), which\n",
        "originally maps the layer inputs to 50,257 dimensions (the size of the vocabulary):\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjD6AIONKS5J"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "num_classes = 2\n",
        "gpt.lm_head = torch.nn.Linear(in_features=NEW_CONFIG[\"emb_dim\"], out_features=num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3KwWikrKS5J"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "Note that in the preceding code, we use NEW_CONFIG[\"emb_dim\"], which is equal to 768 in\n",
        "the \"gpt2-small (124M)\" model, to keep the code below more general.\n",
        "\n",
        "This means we\n",
        "can also use the same code to work with the larger GPT-2 model variants.\n",
        "\n",
        "This new model.out_head output layer has its requires_grad attribute set to True by\n",
        "default, which means that it's the only layer in the model that will be updated during\n",
        "training.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWuPyVJWKS5J"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Additionally, we configure the last transformer block and the final LayerNorm module,\n",
        "which connects this block to the output layer, to be trainable\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71S133K1KS5K"
      },
      "outputs": [],
      "source": [
        "for param in gpt.trf_blocks[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in gpt.final_norm.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiX0OLgJKS5K"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "Even though we added a new output layer and marked certain layers as trainable or nontrainable, we can still use this model in a similar way to previous chapters.\n",
        "\n",
        "For instance, we\n",
        "can feed it an example text identical to how we have done it in earlier chapters. For\n",
        "example, consider the following example text:\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3c-jcD6KS5K",
        "outputId": "b9637fc3-6ba8-438f-99c3-a8f0fdd58fe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: tensor([[5211,  345,  423,  640]], device='cuda:0')\n",
            "Inputs dimensions: torch.Size([1, 4])\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer.encode(\"Do you have time\")\n",
        "inputs = torch.tensor(inputs).unsqueeze(0)\n",
        "print(\"Inputs:\", inputs)\n",
        "print(\"Inputs dimensions:\", inputs.shape) # shape: (batch_size, num_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhpRrDNnKS5L"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Then, we can pass the encoded token IDs to the model as usual:\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20j0COtpsOOl",
        "outputId": "c692d14d-1c26-4400-840c-d8d83c6d9de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt = gpt.to(device)"
      ],
      "metadata": {
        "id": "FjDHxdWcycKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B57Qt_gRKS5L",
        "outputId": "7732ca82-cb2e-4957-d86e-eaaf32b5cdb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outputs:\n",
            " tensor([[[-1.5854,  0.9904],\n",
            "         [-3.7235,  7.4548],\n",
            "         [-2.2661,  6.6049],\n",
            "         [-3.5983,  3.9902]]], device='cuda:0')\n",
            "Outputs dimensions: torch.Size([1, 4, 2])\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = gpt(inputs.to(device))\n",
        "\n",
        "print(\"Outputs:\\n\", outputs)\n",
        "print(\"Outputs dimensions:\", outputs.shape) # shape: (batch_size, num_tokens, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC4V4QODKS5L"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "In earlier chapters, a similar input would have produced an output tensor of [1, 4, 50257],\n",
        "where 50,257 represents the vocabulary size.\n",
        "\n",
        "As in previous chapters, the number of\n",
        "output rows corresponds to the number of input tokens (in this case, 4).\n",
        "\n",
        "However, each\n",
        "output's embedding dimension (the number of columns) is now reduced to 2 instead of\n",
        "50,257 since we replaced the output layer of the model.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI40caDtKS5M"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "Remember that we are interested in finetuning this model so that it returns a class label\n",
        "that indicates whether a model input is spam or not spam.\n",
        "\n",
        "To achieve this, we don't need to\n",
        "finetune all 4 output rows but can focus on a single output token.\n",
        "\n",
        "In particular, we will\n",
        "focus on the last row corresponding to the last output token\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxR2dxJtKS5N"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "To extract the last output token, illustrated in figure 6.11, from the output tensor, we\n",
        "use the following code:    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmEzfL1AKS5O",
        "outputId": "21386b83-fb57-4267-fc82-b1292a55e4b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last output token: tensor([[-3.5983,  3.9902]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(\"Last output token:\", outputs[:, -1, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igeLVlw3KS5Q"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "Having modified the model, the next section will detail the process of transforming the\n",
        "last token into class label predictions and calculate the model's initial prediction accuracy.\n",
        "\n",
        "Following this, we will finetune the model for the spam classification task in the subsequent\n",
        "section.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isatY_KQKS5R"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "So far in this chapter, we have prepared the dataset, loaded a pretrained model, and\n",
        "modified it for classification-finetuning.\n",
        "           \n",
        "Before we proceed with the finetuning itself, only\n",
        "one small part remains: implementing the model evaluation functions used during\n",
        "finetuning,  \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnejKrWiKS5R"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Before implementing the evaluation utilities, let's briefly discuss how we convert the model\n",
        "outputs into class label predictions.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBFchJWaKS5S"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "In the previous chapter, we computed the token ID of the next token generated by the\n",
        "LLM by converting the 50,257 outputs into probabilities via the softmax function and then\n",
        "returning the position of the highest probability via the argmax function.\n",
        "\n",
        "In this chapter, we\n",
        "take the same approach to calculate whether the model outputs a \"spam\" or \"not spam\"\n",
        "prediction for a given input, with the only difference being that we\n",
        "work with 2-dimensional instead of 50,257-dimensional outputs.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWBE4gZpKS5S"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Let's consider the last token output from\n",
        "the previous section:\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdllXl_eKS5S",
        "outputId": "760eea49-bc71-4de7-f3ce-ad1a7756fc76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last output token: tensor([[-3.5983,  3.9902]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(\"Last output token:\", outputs[:, -1, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2gIE6sDKS5T"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "We can obtain the class label via the following code:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsj7RC4TKS5U",
        "outputId": "32b76b1e-8565-4c80-d5de-037f67bebd79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class label: 1\n"
          ]
        }
      ],
      "source": [
        "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
        "label = torch.argmax(probas)\n",
        "print(\"Class label:\", label.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guOaLFtKKS5U"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "In this case, the code returns 1, meaning the model predicts that the input text is \"spam.\"\n",
        "\n",
        "Using the softmax function here is optional because the largest outputs directly correspond\n",
        "to the highest probability scores.\n",
        "\n",
        "Hence, we can simplify the\n",
        "code as follows, without using softmax:\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1lgNU_GKS5U",
        "outputId": "3fc27831-4067-47e2-9058-58697b2d34de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class label: 1\n"
          ]
        }
      ],
      "source": [
        "logits = outputs[:, -1, :]\n",
        "label = torch.argmax(logits)\n",
        "print(\"Class label:\", label.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bd7pktzKS5V"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "This concept can be used to compute the so-called classification accuracy, which measures\n",
        "the percentage of correct predictions across a dataset.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UZGvDqXKS5W"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "To determine the classification accuracy, we apply the argmax-based prediction code to\n",
        "all examples in the dataset and calculate the proportion of correct predictions by defining a\n",
        "calc_accuracy_loader function:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiFnSZuuKS5W"
      },
      "outputs": [],
      "source": [
        "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
        "    model.eval()\n",
        "    correct_predictions, num_examples = 0, 0\n",
        "\n",
        "    if num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
        "            predicted_labels = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            num_examples += predicted_labels.shape[0]\n",
        "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
        "        else:\n",
        "            break\n",
        "    return correct_predictions / num_examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_piOkpaKS5W"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "Let's use the function to determine the classification accuracies across various datasets\n",
        "estimated from 10 batches for efficiency:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "gpt.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
        "\n",
        "torch.manual_seed(123) # For reproducibility due to the shuffling in the training data loader\n",
        "\n",
        "train_accuracy = calc_accuracy_loader(train_loader, gpt, device, num_batches=10)\n",
        "val_accuracy = calc_accuracy_loader(val_loader, gpt, device, num_batches=10)\n",
        "test_accuracy = calc_accuracy_loader(test_loader, gpt, device, num_batches=10)\n",
        "\n",
        "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtYm76aiQ98Q",
        "outputId": "d3dfa4db-d313-4a86-e7fd-e39ed4750b41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy: 46.25%\n",
            "Validation accuracy: 45.00%\n",
            "Test accuracy: 48.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
        "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "WOVh8I-GRhGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ],
      "metadata": {
        "id": "qM4aOMsFSPQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
        "    train_loss = calc_loss_loader(train_loader, gpt, device, num_batches=5)\n",
        "    val_loss = calc_loss_loader(val_loader, gpt, device, num_batches=5)\n",
        "    test_loss = calc_loss_loader(test_loader, gpt, device, num_batches=5)\n",
        "\n",
        "print(f\"Training loss: {train_loss:.3f}\")\n",
        "print(f\"Validation loss: {val_loss:.3f}\")\n",
        "print(f\"Test loss: {test_loss:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QfarGeASbGI",
        "outputId": "e6fd1990-ce45-4124-f86e-8bf0d09a6903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 2.453\n",
            "Validation loss: 2.583\n",
            "Test loss: 2.322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iZrXyFGKS5e"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "In this section, we define and use the training function to finetune the pretrained LLM and\n",
        "improve its spam classification accuracy.\n",
        "    \n",
        "The training loop is the\n",
        "same overall training loop we used earlier, with the only difference being that we\n",
        "calculate the classification accuracy instead of generating a sample text for evaluating the\n",
        "model.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmJwTpI7KS5e"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "The training function also closely mirrors\n",
        "the train_model_simple function used for pretraining the model earlier.\n",
        "                                    \n",
        "The only two distinctions are that we now track the number of training examples seen\n",
        "(examples_seen) instead of the number of tokens, and we calculate the accuracy after each\n",
        "epoch instead of printing a sample text:\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FTWTTOxKS5f"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "Step 1: Set model to training mode\n",
        "\n",
        "Step 2: Reset loss gradients from previous batch iteration\n",
        "\n",
        "Step 3: Calculate loss gradients\n",
        "\n",
        "Step 4: Update model weights using loss gradients\n",
        "\n",
        "Step 5: New: track examples instead of tokens\n",
        "\n",
        "Step 6: Optional evaluation step\n",
        "\n",
        "Step 7: Calculate accuracy after each epoch\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ],
      "metadata": {
        "id": "_rKjDJjXTLLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Overall the same as `train_model_simple` in chapter 5\n",
        "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                            eval_freq, eval_iter):\n",
        "    # Initialize lists to track losses and examples seen\n",
        "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "    examples_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            examples_seen += input_batch.shape[0] # New: track examples instead of tokens\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Calculate accuracy after each epoch\n",
        "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
        "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "        train_accs.append(train_accuracy)\n",
        "        val_accs.append(val_accuracy)\n",
        "\n",
        "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
      ],
      "metadata": {
        "id": "BrmYirZvS7Ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "optimizer = torch.optim.AdamW(gpt.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 5\n",
        "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
        "    gpt, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHSZBPt4TUSg",
        "outputId": "b9940119-0a5b-4cd6-a3a2-d781902be7ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 2.158, Val loss 2.397\n",
            "Ep 1 (Step 000050): Train loss 0.855, Val loss 0.879\n",
            "Ep 1 (Step 000100): Train loss 0.611, Val loss 0.722\n",
            "Training accuracy: 90.00% | Validation accuracy: 85.00%\n",
            "Ep 2 (Step 000150): Train loss 0.613, Val loss 0.618\n",
            "Ep 2 (Step 000200): Train loss 0.698, Val loss 0.648\n",
            "Ep 2 (Step 000250): Train loss 0.535, Val loss 0.622\n",
            "Training accuracy: 57.50% | Validation accuracy: 57.50%\n",
            "Ep 3 (Step 000300): Train loss 0.440, Val loss 0.536\n",
            "Ep 3 (Step 000350): Train loss 0.431, Val loss 0.468\n",
            "Training accuracy: 72.50% | Validation accuracy: 65.00%\n",
            "Ep 4 (Step 000400): Train loss 0.286, Val loss 0.540\n",
            "Ep 4 (Step 000450): Train loss 0.467, Val loss 0.416\n",
            "Ep 4 (Step 000500): Train loss 0.422, Val loss 0.479\n",
            "Training accuracy: 85.00% | Validation accuracy: 80.00%\n",
            "Ep 5 (Step 000550): Train loss 0.368, Val loss 0.377\n",
            "Ep 5 (Step 000600): Train loss 0.401, Val loss 0.401\n",
            "Training accuracy: 85.00% | Validation accuracy: 80.00%\n",
            "Training completed in 0.96 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkkZnEivKS5j"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
        "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(label.capitalize())\n",
        "    ax1.legend()\n",
        "\n",
        "    # Create a second x-axis for examples seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(examples_seen, train_values, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Examples seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(f\"{label}-plot.pdf\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlaaumgoKS5k",
        "outputId": "b83519be-0a03-4d7d-9867-a31e4315b2a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
        "\n",
        "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# without optional evaluation\n",
        "\n",
        "# Overall the same as `train_model_simple` in chapter 5\n",
        "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                            eval_freq, eval_iter):\n",
        "    # Initialize lists to track losses and examples seen\n",
        "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "    examples_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            examples_seen += input_batch.shape[0] # New: track examples instead of tokens\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            # if global_step % eval_freq == 0:\n",
        "            #     train_loss, val_loss = evaluate_model(\n",
        "            #         model, train_loader, val_loader, device, eval_iter)\n",
        "            #     train_losses.append(train_loss)\n",
        "            #     val_losses.append(val_loss)\n",
        "            #     print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "            #           f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Calculate accuracy after each epoch\n",
        "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
        "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "        train_accs.append(train_accuracy)\n",
        "        val_accs.append(val_accuracy)\n",
        "\n",
        "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
      ],
      "metadata": {
        "id": "zfNFUGOjiEK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "optimizer = torch.optim.AdamW(gpt.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 1\n",
        "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
        "    gpt, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cda97ae1-b248-4b97-eb29-cc48b41e8b30",
        "id": "dxA0MMnZiEK-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy: 85.00% | Validation accuracy: 82.50%\n",
            "Training completed in 0.15 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt.eval()\n",
        "test_accuracy = calc_accuracy_loader(test_loader, gpt, device)\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfI6_k4OiXjn",
        "outputId": "5818f796-410f-4174-eb57-2913680ae34e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 84.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt.eval()\n",
        "with torch.no_grad():\n",
        "    test_loss = calc_loss_loader(test_loader, gpt, device)\n",
        "print(f\"Test loss: {test_loss:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5ZVaB9ulNJ4",
        "outputId": "8f2d1db7-597f-4d28-bffd-ba2e2fac0c9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0zlzqkyKS5n"
      },
      "outputs": [],
      "source": [
        "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare inputs to the model\n",
        "    input_ids = tokenizer.encode(text)\n",
        "    supported_context_length = model.positional_embedding.weight.shape[0]\n",
        "    # Note: In the book, this was originally written as pos_emb.weight.shape[1] by mistake\n",
        "    # It didn't break the code but would have caused unnecessary truncation (to 768 instead of 1024)\n",
        "\n",
        "    # Truncate sequences if they too long\n",
        "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
        "\n",
        "    # Pad sequences to the longest sequence\n",
        "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
        "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0) # add batch dimension\n",
        "\n",
        "    # Model inference\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)[:, -1, :]  # Logits of the last output token\n",
        "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    # Return the classified result\n",
        "    return \"spam\" if predicted_label == 1 else \"not spam\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDYucu76KS5o",
        "outputId": "e8291926-7c71-4509-ee7d-753ce8cd470b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spam\n"
          ]
        }
      ],
      "source": [
        "text_1 = (\n",
        "    \"You are a winner you have been specially\"\n",
        "    \" selected to receive $1000 cash or a $2000 award.\"\n",
        ")\n",
        "\n",
        "print(classify_review(\n",
        "    text_1, gpt, tokenizer, device, max_length=train_dataset.max_length\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DW0lwZdWKS5o",
        "outputId": "a80729e4-5011-44e0-d399-edcdcca99b42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not spam\n"
          ]
        }
      ],
      "source": [
        "text_2 = (\n",
        "    \"Hey, just wanted to check if we're still on\"\n",
        "    \" for dinner tonight? Let me know!\"\n",
        ")\n",
        "\n",
        "print(classify_review(\n",
        "    text_2, gpt, tokenizer, device, max_length=train_dataset.max_length\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(gpt.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "torch.save({\n",
        "    \"model_state_dict\": gpt.state_dict(),\n",
        "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "    },\n",
        "    \"model_and_optimizer.pth\"\n",
        ")"
      ],
      "metadata": {
        "id": "gWwGVd_cow13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Created by **Md. Shadikur Rahman Sheam**.\n",
        "- [Linkedin](https://www.linkedin.com/in/md-shadikur-rahman-sheam-3826482b3/)\n",
        "- [Github](https://github.com/sadikurSenpai)\n",
        "- [Facebook](https://www.facebook.com/profile.php?id=100091833665881)"
      ],
      "metadata": {
        "id": "xToCzBiAYtLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Special thanks to Sebastian Raschka for his amazing book."
      ],
      "metadata": {
        "id": "m3qvNTEQY2i0"
      }
    }
  ]
}