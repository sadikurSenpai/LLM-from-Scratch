{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Hello everyone! So far on this journey, we’ve managed to build our very own version of GPT-2, and later brought in the open-source weights provided by OpenAI. We walked through the idea of pretraining—imagining what it would take to train on massive datasets for countless epochs with enormous resources—before grounding ourselves with the already available pretrained weights. From there, we successfully fine-tuned the model to work as an email classifier, giving us our first real taste of adapting a large language model to a specific task.\n",
        "\n",
        "Now, we’re ready to take the next step: instruction fine-tuning. This is another widely used and practical approach to tailoring LLMs. And the good news is, since we’ve already built a strong understanding of the key concepts and mechanisms, this next stage will feel familiar. It’s just the natural continuation of our journey—one more piece of the puzzle in shaping these models to follow human instructions more effectively."
      ],
      "metadata": {
        "id": "UbJoCAxbnI7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Preparation"
      ],
      "metadata": {
        "id": "lfgwiiH2m2qy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J5zSSB3HNV_",
        "outputId": "9a8b6126-9a2b-4a17-ca2d-a21b9dabe582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of entries: 1100\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import urllib\n",
        "import ssl\n",
        "\n",
        "def download_and_load_file(file_path, url):\n",
        "    ssl_context = ssl.create_default_context()\n",
        "    ssl_context.check_hostname = False\n",
        "    ssl_context.verify_mode = ssl.CERT_NONE\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        with urllib.request.urlopen(url, context=ssl_context) as response:\n",
        "            text_data = response.read().decode(\"utf-8\")\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(text_data)\n",
        "    else:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            text_data = file.read()\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "file_path = \"instruction-data.json\"\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
        "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
        ")\n",
        "\n",
        "data = download_and_load_file(file_path, url)\n",
        "print(\"Number of entries:\", len(data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXGOVUfGMWrs",
        "outputId": "41383451-b479-49cb-9cc8-6ede48f0cda2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'instruction': 'Edit the following sentence for grammar.',\n",
              " 'input': 'He go to the park every day.',\n",
              " 'output': 'He goes to the park every day.'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When it comes to instruction fine-tuning, the way we prepare our data is just as important as the training itself. One of the most effective approaches comes from Stanford’s Alpaca project, where the team experimented with several methods of formatting instruction-based data. After trying different styles and variations, they discovered a particular structure that consistently gave the best results—this has since become widely known as the Alpaca format.\n",
        "\n",
        "By organizing the data in this way, the model learns not only to generate text but also to follow human-like instructions more reliably. This format has been tested and proven to work exceptionally well, which is why we’ll be adopting it in our own instruction fine-tuning journey.\n",
        "\n",
        "Here is the link of Alpaca format which you can read: https://github.com/tatsu-lab/stanford_alpaca"
      ],
      "metadata": {
        "id": "b3Ifx4NNpJVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alpaca format:"
      ],
      "metadata": {
        "id": "4JThxBxPp0ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "# ### Instruction:\n",
        "# {instruction}\n",
        "\n",
        "# ### Input:\n",
        "# {input}\n",
        "\n",
        "# ### Response:"
      ],
      "metadata": {
        "id": "eIAnPAxCNsVO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_input(entry):\n",
        "  instruction_text = (\n",
        "      f\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "      f\"Write a response that appropriately completes the request.\"\n",
        "      f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "      )\n",
        "  input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "  return instruction_text + input_text"
      ],
      "metadata": {
        "id": "isYef2jqMn_Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(format_input(data[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLCjeBLRPXlL",
        "outputId": "c1372e27-7aa0-478e-f14b-6e5ec41aa3d3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Edit the following sentence for grammar.\n",
            "\n",
            "### Input:\n",
            "He go to the park every day.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(format_input(data[999]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABBuW4piN-Ow",
        "outputId": "380e2ff6-2de2-4717-80ad-3efdfc6c3e6f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is an antonym of 'complicated'?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_input = format_input(data[1])\n",
        "desired_response = f\"\\n\\n### Response:\\n{data[1]['output']}\"\n",
        "\n",
        "print(model_input + desired_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojWJDm9nOAwl",
        "outputId": "67861276-a5b1-4c6a-9ca5-cd1437232dbd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Edit the following sentence for grammar.\n",
            "\n",
            "### Input:\n",
            "He go to the park every day.\n",
            "\n",
            "### Response:\n",
            "He goes to the park every day.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Splitting:"
      ],
      "metadata": {
        "id": "LoJGmBHpp6JH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FJZS7ersMLTk"
      },
      "outputs": [],
      "source": [
        "train_portion = int(len(data) * 0.85)  # 85% for training\n",
        "test_portion = int(len(data) * 0.1)    # 10% for testing\n",
        "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
        "\n",
        "train_data = data[:train_portion]\n",
        "test_data = data[train_portion:train_portion + test_portion]\n",
        "val_data = data[train_portion + test_portion:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bXwsjmIjMLTk",
        "outputId": "987589c8-b5de-4111-ed19-0f8a426134fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set length: 935\n",
            "Validation set length: 55\n",
            "Test set length: 110\n"
          ]
        }
      ],
      "source": [
        "print(\"Training set length:\", len(train_data))\n",
        "print(\"Validation set length:\", len(val_data))\n",
        "print(\"Test set length:\", len(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD-gYjA3P2xb",
        "outputId": "61147c58-6575-4be3-fac4-ec729e2886fd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1100"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "\n",
        "        # Pre-tokenize texts\n",
        "        self.encoded_texts = []\n",
        "        for entry in data:\n",
        "            instruction_plus_input = format_input(entry)\n",
        "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "            full_text = instruction_plus_input + response_text\n",
        "            self.encoded_texts.append(\n",
        "                tokenizer.encode(full_text)\n",
        "            )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.encoded_texts[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "fz37MNxJzvd2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "UgTf4yXC7ygf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    ignore_index=-100,\n",
        "    allowed_max_length=None,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # Find the longest sequence in the batch\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs and targets\n",
        "    inputs_lst, targets_lst = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
        "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "\n",
        "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
        "        mask = targets == pad_token_id\n",
        "        indices = torch.nonzero(mask).squeeze()\n",
        "        if indices.numel() > 1:\n",
        "            targets[indices[1:]] = ignore_index\n",
        "\n",
        "        # New: Optionally truncate to maximum sequence length\n",
        "        if allowed_max_length is not None:\n",
        "            inputs = inputs[:allowed_max_length]\n",
        "            targets = targets[:allowed_max_length]\n",
        "\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    # Convert list of inputs and targets to tensors and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "\n",
        "    return inputs_tensor, targets_tensor"
      ],
      "metadata": {
        "id": "3Ix7ctpX8Fv_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When working with instruction fine-tuning, the way we prepare batches of data for training becomes critical. Unlike simple classification tasks, here we’re dealing with variable-length sequences of text (instructions, inputs, and outputs). If we simply feed them as-is, the model will face issues—different sequence lengths don’t align well in a batch, and the loss function may mistakenly compute gradients on padding tokens.\n",
        "\n",
        "That’s why we wrote this *custom collate_fn*. Let’s break down what it does and why it matters:\n",
        "**Dynamic Padding:**\n",
        "- Each sequence can be a different length.\n",
        "- We find the longest one and pad the others to match.\n",
        "- Padding is done with <|endoftext|> (not zeros) to stay consistent with GPT-2’s tokenizer.\n",
        "\n",
        "**Input–Target Alignment**\n",
        "- Language models learn by predicting the next token.\n",
        "- So, we prepare: Inputs → all tokens except the last one. Targets → all tokens except the first one (shifted by 1).\n",
        "\n",
        "**Masking Padding**\n",
        "- Predicting padding is useless.\n",
        "- We replace padding tokens in the targets with ignore_index=-100.\n",
        "- PyTorch’s CrossEntropyLoss will skip these positions, focusing only on real tokens."
      ],
      "metadata": {
        "id": "PKjAb3N1s5lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3\n",
        ")\n",
        "\n",
        "inputs, targets = custom_collate_fn(batch)\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNCGAMT48H6X",
        "outputId": "c79cdb67-37b9-4f15-a1ab-1140c5cc173a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n",
            "tensor([[    1,     2,     3,     4, 50256],\n",
            "        [    6, 50256,  -100,  -100,  -100],\n",
            "        [    8,     9, 50256,  -100,  -100]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paw7cosr7k8W"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "The modified collate function works as expected, altering the target list by inserting the\n",
        "token ID -100.\n",
        "\n",
        "What is the logic behind this adjustment? Let's explore the underlying\n",
        "purpose of this modification.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjoGSWRT7k8W"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "For demonstration purposes, consider the following simple and self-contained example\n",
        "where each output logit can correspond to a potential token from the model's vocabulary.\n",
        "\n",
        "Here's how we might calculate the cross entropy loss during\n",
        "training when the model predicts a sequence of tokens, similar to what we have done in\n",
        "chapter 5 when pretraining the model, or in chapter 6 when finetuning the model for\n",
        "classification:\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "q3XoSlGc7k8W",
        "outputId": "00e3c3c7-0280-48e5-95d2-3e905a69a762",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.1269)\n"
          ]
        }
      ],
      "source": [
        "logits_1 = torch.tensor(\n",
        "    [[-1.0, 1.0],  # 1st training example\n",
        "     [-0.5, 1.5]]  # 2nd training example\n",
        ")\n",
        "targets_1 = torch.tensor([0, 1])\n",
        "\n",
        "\n",
        "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
        "print(loss_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Begp3WZu7k8W"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Adding an additional token ID will, as we would expect, affect the loss calculation.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4dgureKF7k8W",
        "outputId": "59609bda-b082-4f58-a7c2-a779e6250cc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.7936)\n"
          ]
        }
      ],
      "source": [
        "logits_2 = torch.tensor(\n",
        "    [[-1.0, 1.0],\n",
        "     [-0.5, 1.5],\n",
        "     [-0.5, 1.5]]  # New 3rd training example\n",
        ")\n",
        "targets_2 = torch.tensor([0, 1, 1])\n",
        "\n",
        "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
        "print(loss_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxp7o5Mh7k8W"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Now, let's get to the interesting part and see what happens if we replace the third target\n",
        "token ID with -100:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "v9BIYnpx7k8X",
        "outputId": "66961137-d530-4338-e675-2e8841305780",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.1269)\n",
            "loss_1 == loss_3: tensor(True)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "targets_3 = torch.tensor([0, 1, -100])\n",
        "\n",
        "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
        "print(loss_3)\n",
        "print(\"loss_1 == loss_3:\", loss_1 == loss_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj1pgLqD7k8X"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "Based on this result, we can see that the resulting loss on these 3 training examples is\n",
        "identical to the loss we calculated from the 2 training examples earlier.\n",
        "\n",
        "In other words, the\n",
        "cross entropy loss function ignored the third entry in the targets_3 vector, the token ID\n",
        "corresponding to -100.\n",
        "\n",
        "(Interested readers can try to replace the -100 value with another\n",
        "token IDs that is not 0 or 1, and will see that this results in an error.)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foTvEk277k8X"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "So, what's so special about -100 that it's ignored by the cross entropy loss? The default\n",
        "setting of the cross entropy function in PyTorch is cross_entropy(...,\n",
        "ignore_index=-100).\n",
        "\n",
        "This means that it ignores targets labeled with -100.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdu52rJs7k8X"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "In this chapter, we take advantage of this ignore_index to ignore the additional end-oftext (padding) tokens that we used to pad the training examples to have the same length in\n",
        "each batch.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f511Qfs7k8X"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "\n",
        "However, we want to keep one 50256 (end-of-text)\n",
        "token ID in the targets because it helps the LLM to learn to generate end-of-text tokens,\n",
        "which we can use as an indicator that a response is complete.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNbEtmK-7k8X"
      },
      "source": [
        "**MASKING TARGET TOKEN IDS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKkzrMpc7k8X"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "In addition to masking out padding tokens, it is also common to mask out the target\n",
        "token IDs that correspond to the instruction\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baaJri257k8X"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "By masking out the target token IDs that correspond to the instruction, the LLM cross entropy loss is only computed for the generated response target\n",
        "IDs.\n",
        "\n",
        "By masking out the instruction tokens, the model is trained to focus on generating\n",
        "accurate responses rather than additionally also memorizing instructions, which can help\n",
        "with reducing overfitting.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtDx3ZwC7k8X"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "Currently, researchers are divided on whether masking the instructions is universally beneficial during instruction finetuning.\n",
        "\n",
        "For instance, a recent\n",
        "paper titled \"Instruction Tuning With Loss Over Instructions\" demonstrated that not\n",
        "masking the instructions benefits the LLM performance.\n",
        "\n",
        "In this chapter, we do not apply masking and leave it as an optional\n",
        "exercise for the reader.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "syWAUailAOPj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EiIiQ5LAORx",
        "outputId": "62171f80-8060-4320-e5b6-0cd3969d8dcb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_dataset = InstructionDataset(train_data, tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=custom_collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "val_dataset = InstructionDataset(val_data, tokenizer)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=custom_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "test_dataset = InstructionDataset(test_data, tokenizer)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=custom_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")"
      ],
      "metadata": {
        "id": "7Yf1yuvoAtlH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input, target in train_loader:\n",
        "  print(input.shape, target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UW5b-u1eA3Iw",
        "outputId": "df5346d6-5728-493c-d3d3-8eebdb3ff6ad"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 85]) torch.Size([8, 85])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 84]) torch.Size([8, 84])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 86]) torch.Size([8, 86])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 92]) torch.Size([8, 92])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 85]) torch.Size([8, 85])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 84]) torch.Size([8, 84])\n",
            "torch.Size([8, 98]) torch.Size([8, 98])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 97]) torch.Size([8, 97])\n",
            "torch.Size([8, 92]) torch.Size([8, 92])\n",
            "torch.Size([8, 92]) torch.Size([8, 92])\n",
            "torch.Size([8, 79]) torch.Size([8, 79])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 85]) torch.Size([8, 85])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 84]) torch.Size([8, 84])\n",
            "torch.Size([8, 92]) torch.Size([8, 92])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 96]) torch.Size([8, 96])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 79]) torch.Size([8, 79])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 79]) torch.Size([8, 79])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 90]) torch.Size([8, 90])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 92]) torch.Size([8, 92])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 86]) torch.Size([8, 86])\n",
            "torch.Size([8, 100]) torch.Size([8, 100])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 84]) torch.Size([8, 84])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 92]) torch.Size([8, 92])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn-_ZDtgBHcw",
        "outputId": "0cb8c42d-a244-4112-a8f2-e63459e23e11"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "116"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wI9LPlgBhCf",
        "outputId": "e5aab69c-8d9b-494d-ef71-1bfd2cad85e0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRhra_w0BmdQ",
        "outputId": "a45a13e4-4fa8-48d8-c052-6f1226b07bd5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())"
      ],
      "metadata": {
        "id": "zcIzxRon_MnC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "hzxAkzl08EP3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_355M = {\n",
        "    'vocab_size': 50257,      # GPT-2 vocab size\n",
        "    'emb_dim': 1024,          # Hidden size\n",
        "    'context_length': 1024,   # Max sequence length\n",
        "    'n_heads': 16,            # Attention heads\n",
        "    'n_layers': 24,           # Number of transformer blocks\n",
        "    'drop_rate': 0.1,         # Dropout (same as small)\n",
        "    'qkv_bias': True,         # Bias terms in QKV projections\n",
        "    'model_name': 'gpt2-medium'\n",
        "}\n"
      ],
      "metadata": {
        "id": "cBhdPpi08AUn"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))"
      ],
      "metadata": {
        "id": "-x5AgXcU8AUm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "coqPWhoG8AUm"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift"
      ],
      "metadata": {
        "id": "gpXldXK58AUn"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "2gHupfu68AUn"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "      super().__init__() # d_in, d_out, context_length, dropout, num_heads, qkv_bias=False\n",
        "      self.attn = MultiHeadAttention(cfg['emb_dim'], cfg['emb_dim'], cfg['context_length'], cfg['drop_rate'], cfg['n_heads'], cfg['qkv_bias'])\n",
        "      self.ffn = FeedForward(cfg)\n",
        "      self.norm1 = nn.LayerNorm(cfg['emb_dim'])\n",
        "      self.norm2 = nn.LayerNorm(cfg['emb_dim'])\n",
        "      self.dropout = nn.Dropout(cfg['drop_rate'])\n",
        "\n",
        "  def forward(self, x):\n",
        "      shortcut = x\n",
        "      x = self.norm1(x)\n",
        "      x = self.attn(x)\n",
        "      x = self.dropout(x)\n",
        "      x = shortcut + x\n",
        "\n",
        "      shortcut = x\n",
        "      x = self.norm2(x)\n",
        "      x = self.ffn(x)\n",
        "      x = self.dropout(x)\n",
        "      x = shortcut + x\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "C9EmtHI28AUn"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "      super().__init__()\n",
        "      self.token_embedding = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
        "      self.positional_embedding = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
        "      self.dropout = nn.Dropout(cfg['drop_rate'])\n",
        "\n",
        "      self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "      self.final_norm = nn.LayerNorm(cfg['emb_dim'])\n",
        "      self.lm_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "      batch_size, seq_len = x.shape\n",
        "      token_embeddings = self.token_embedding(x)\n",
        "      position_embeddings = self.positional_embedding(torch.arange(seq_len, device=x.device))\n",
        "      input_embeddings = token_embeddings + position_embeddings\n",
        "      x = self.dropout(input_embeddings)\n",
        "      x = self.trf_blocks(x)\n",
        "      x = self.final_norm(x)\n",
        "      x = self.lm_head(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "KdGl6iyL8AUn"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:].to(idx.device) # Explicitly move to device\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "J3c0Msgc8F7g"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Bi-nmpFnK4Oh"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow>=2.15.0 tqdm>=4.66"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "rYvICBrsK4Oh",
        "outputId": "46b2c793-e84d-4688-de2f-532f5b31585e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "tqdm version: 4.67.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"tqdm version:\", tqdm.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right, device=left.device))"
      ],
      "metadata": {
        "id": "Iu2CXJwBDc25"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.positional_embedding.weight = assign(gpt.positional_embedding.weight, params['wpe'])\n",
        "    gpt.token_embedding.weight = assign(gpt.token_embedding.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].attn.W_query.weight = assign(\n",
        "            gpt.trf_blocks[b].attn.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].attn.W_key.weight = assign(\n",
        "            gpt.trf_blocks[b].attn.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].attn.W_value.weight = assign(\n",
        "            gpt.trf_blocks[b].attn.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].attn.W_query.bias = assign(\n",
        "            gpt.trf_blocks[b].attn.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].attn.W_key.bias = assign(\n",
        "            gpt.trf_blocks[b].attn.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].attn.W_value.bias = assign(\n",
        "            gpt.trf_blocks[b].attn.W_value.bias, v_b)\n",
        "\n",
        "        gpt.trf_blocks[b].attn.out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].attn.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].attn.out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].attn.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].ffn.layers[0].weight = assign(\n",
        "            gpt.trf_blocks[b].ffn.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ffn.layers[0].bias = assign(\n",
        "            gpt.trf_blocks[b].ffn.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ffn.layers[2].weight = assign(\n",
        "            gpt.trf_blocks[b].ffn.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ffn.layers[2].bias = assign(\n",
        "            gpt.trf_blocks[b].ffn.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].norm1.weight = assign(\n",
        "            gpt.trf_blocks[b].norm1.weight,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm1.bias = assign(\n",
        "            gpt.trf_blocks[b].norm1.bias,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm2.weight = assign(\n",
        "            gpt.trf_blocks[b].norm2.weight,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm2.bias = assign(\n",
        "            gpt.trf_blocks[b].norm2.bias,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "\n",
        "    gpt.final_norm.weight = assign(gpt.final_norm.weight, params[\"g\"])\n",
        "    gpt.final_norm.bias = assign(gpt.final_norm.bias, params[\"b\"])\n",
        "    gpt.lm_head.weight = assign(gpt.lm_head.weight, params[\"wte\"])"
      ],
      "metadata": {
        "id": "ukX1TFcmD7zZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_download3 import download_and_load_gpt2\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"drop_rate\": 0.0,        # Dropout rate\n",
        "    \"qkv_bias\": True         # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
        "\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "\n",
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "settings, params = download_and_load_gpt2(\n",
        "    model_size=model_size,\n",
        "    models_dir=\"gpt2\"\n",
        ")\n",
        "\n",
        "model = GPTModel(BASE_CONFIG).to(device)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "1_Dm4M2rjEkx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "098e2ad6-acee-4955-e220-053609a52131"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 188kiB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.20MiB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 149kiB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [00:42<00:00, 11.7MiB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 8.01MiB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 1.40MiB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 1.35MiB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 173kiB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.31MiB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "hparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 148kiB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 1.42G/1.42G [01:47<00:00, 13.2MiB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.index: 100%|██████████| 10.4k/10.4k [00:00<00:00, 14.2MiB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.meta: 100%|██████████| 927k/927k [00:00<00:00, 2.58MiB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 1.58MiB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (token_embedding): Embedding(50257, 1024)\n",
              "  (positional_embedding): Embedding(1024, 1024)\n",
              "  (dropout): Dropout(p=0.0, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (12): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (13): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (14): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (15): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (16): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (17): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (18): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (19): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (20): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (21): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (22): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (23): TransformerBlock(\n",
              "      (attn): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffn): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model.eval()\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids('Every effort moves you', tokenizer).to(device),\n",
        "    max_new_tokens=50,\n",
        "    context_size=BASE_CONFIG[\"context_length\"],\n",
        "    top_k=50,\n",
        "    temperature=0.0,\n",
        "    eos_id=50256\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "id": "F11IBhatEThw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "277f282f-a214-4367-de1c-abb680b7dbcd"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you forward, but you must be careful. You must not let your guard down. You must not let your guard down.\"\n",
            "\n",
            "\"I will not let you down.\"\n",
            "\n",
            "\"I will not let you down.\"\n",
            "\n",
            "\"I will not\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(format_input(val_data[0]))"
      ],
      "metadata": {
        "id": "gmIiRtdIicI7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54add800-fbdf-4ebb-dfc0-4522cc73b89c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model.eval()\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(format_input(val_data[0]), tokenizer).to(device),\n",
        "    max_new_tokens=256,\n",
        "    context_size=BASE_CONFIG[\"context_length\"],\n",
        "    top_k=50,\n",
        "    temperature=0.0,\n",
        "    eos_id=50256\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "id": "QOmyKDNrig2S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4671a77-1c40-440b-e2e3-2a6f90ed38c1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
            "\n",
            "### Input:\n",
            "\n",
            "The active sentence: 'The chef cooks the meal every day.'\n",
            "\n",
            "### Output:\n",
            "\n",
            "The passive sentence: 'The chef cooks the meal every day.'\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "response_text = generated_text[len(format_input(val_data[0])):].strip()\n",
        "print(response_text)"
      ],
      "metadata": {
        "id": "WxBijkDSk_bK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b90372eb-706c-40e1-ebb6-7b166e6d847e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Input:\n",
            "\n",
            "The active sentence: 'The chef cooks the meal every day.'\n",
            "\n",
            "### Output:\n",
            "\n",
            "The passive sentence: 'The chef cooks the meal every day.'\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Example:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches\n",
        "\n",
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
        "            global_step += 1\n",
        "\n",
        "            if global_step % 15 == 0:\n",
        "                print(f\"The training is going on for batch {global_step} in epoch {epoch+1}\")\n",
        "\n",
        "            # # Optional evaluation step\n",
        "            # if global_step % eval_freq == 0:\n",
        "            #     train_loss, val_loss = evaluate_model(\n",
        "            #         model, train_loader, val_loader, device, eval_iter)\n",
        "            #     train_losses.append(train_loss)\n",
        "            #     val_losses.append(val_loss)\n",
        "            #     track_tokens_seen.append(tokens_seen)\n",
        "            #     print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "            #           f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        model.eval()\n",
        "        print(token_ids_to_text(generate(model, text_to_token_ids(start_context, tokenizer).to(device),\n",
        "                 max_new_tokens=50, context_size=BASE_CONFIG[\"context_length\"],\n",
        "                 top_k=50, temperature=0), tokenizer))\n",
        "\n",
        "    # return train_losses, val_losses, track_tokens_seen\n"
      ],
      "metadata": {
        "id": "yev4vpuEllW4"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ],
      "metadata": {
        "id": "7quSJZualrdD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e1aebc7-2104-49d7-e816-acf0256eedce"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 3.8396378993988036\n",
            "Validation loss: 3.7788986682891847\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "3tLfY3N7ly8S",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "VatoDBkaBdq_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bd1eed3-9a6e-458f-8bbf-ec10e53908a5"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have run the code below actually 2 times, so you can imagine the epochs as 4"
      ],
      "metadata": {
        "id": "nke_HkBmrGld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
        "epochs = 2\n",
        "\n",
        "train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "id": "XzY-Z8nol2iq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccac839f-ad19-4745-ab32-e5119519f329"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The training is going on for batch 0 in epoch 1\n",
            "The training is going on for batch 15 in epoch 1\n",
            "The training is going on for batch 30 in epoch 1\n",
            "The training is going on for batch 45 in epoch 1\n",
            "The training is going on for batch 60 in epoch 1\n",
            "The training is going on for batch 75 in epoch 1\n",
            "The training is going on for batch 90 in epoch 1\n",
            "The training is going on for batch 105 in epoch 1\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
            "\n",
            "### Response:\n",
            "The meal is prepared by the chef.<|endoftext|>The following is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What\n",
            "The training is going on for batch 120 in epoch 2\n",
            "The training is going on for batch 135 in epoch 2\n",
            "The training is going on for batch 150 in epoch 2\n",
            "The training is going on for batch 165 in epoch 2\n",
            "The training is going on for batch 180 in epoch 2\n",
            "The training is going on for batch 195 in epoch 2\n",
            "The training is going on for batch 210 in epoch 2\n",
            "The training is going on for batch 225 in epoch 2\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
            "\n",
            "### Response:\n",
            "The meal is prepared by the chef every day.<|endoftext|>The following is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Training completed in 2.20 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader, model, device)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device)\n",
        "    test_loss = calc_loss_loader(test_loader, model, device)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)\n",
        "print(\"Test loss:\", test_loss)"
      ],
      "metadata": {
        "id": "YHJeQ8hCCs5q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "632ced0e-28e1-456c-dbad-7168ae7c152a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.17877831811021114\n",
            "Validation loss: 0.5765614424433027\n",
            "Test loss: 0.6692315489053726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_text = token_ids_to_text(generate(model, text_to_token_ids(format_input(val_data[0]), tokenizer).to(device),\n",
        "                 max_new_tokens=50, context_size=BASE_CONFIG[\"context_length\"],\n",
        "                 top_k=50, temperature=0, eos_id=50256), tokenizer)\n",
        "\n",
        "response_text = response_text[len(format_input(val_data[0])):].strip()\n",
        "print(response_text)"
      ],
      "metadata": {
        "id": "6oZaeuM0DNa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "458c2df1-7ae6-4047-9043-a1fbd535fd75"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Response:\n",
            "The meal is prepared by the chef every day.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"\n",
        "torch.save(model.state_dict(), file_name)\n",
        "print(f\"Model saved as {file_name}\")\n",
        "\n",
        "# Load model via\n",
        "# model.load_state_dict(torch.load(\"gpt2-medium355M-sft.pth\"))"
      ],
      "metadata": {
        "id": "HoTXDV9OHTL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5b3af98-aa7c-4515-a904-4a8cd7862c15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for entry in test_data[:3]:\n",
        "\n",
        "    input_text = format_input(entry)\n",
        "\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    response_text = (\n",
        "        generated_text[len(input_text):]\n",
        "        .replace(\"### Response:\", \"\")\n",
        "        .strip()\n",
        ")\n",
        "\n",
        "    print(input_text)\n",
        "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
        "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
        "    print(\"-------------------------------------\")"
      ],
      "metadata": {
        "id": "qqBh8gavL_Fm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "487e49bd-b793-41f9-b375-16a421453ef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Rewrite the sentence using a simile.\n",
            "\n",
            "### Input:\n",
            "The car is very fast.\n",
            "\n",
            "Correct response:\n",
            ">> The car is as fast as lightning.\n",
            "\n",
            "Model response:\n",
            ">> The car is as fast as a cheetah.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What type of cloud is typically associated with thunderstorms?\n",
            "\n",
            "Correct response:\n",
            ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
            "\n",
            "Model response:\n",
            ">> The type of cloud typically associated with thunderstorms is a cumulus.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Name the author of 'Pride and Prejudice'.\n",
            "\n",
            "Correct response:\n",
            ">> Jane Austen.\n",
            "\n",
            "Model response:\n",
            ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
        "\n",
        "    input_text = format_input(entry)\n",
        "\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
        "\n",
        "    test_data[i][\"model_response\"] = response_text\n",
        "\n",
        "\n",
        "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
        "    json.dump(test_data, file, indent=4)  # \"indent\" for pretty-printing"
      ],
      "metadata": {
        "id": "3_pWhLCCXjf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for entry in test_data[:110]:\n",
        "\n",
        "    input_text = format_input(entry)\n",
        "\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    response_text = (\n",
        "        generated_text[len(input_text):]\n",
        "        .replace(\"### Response:\", \"\")\n",
        "        .strip()\n",
        ")\n",
        "\n",
        "    print(input_text)\n",
        "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
        "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
        "    print(\"-------------------------------------\")"
      ],
      "metadata": {
        "id": "wIMfbGnAZRj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c5ee4ac-661a-4c77-be0f-79a399da69ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Rewrite the sentence using a simile.\n",
            "\n",
            "### Input:\n",
            "The car is very fast.\n",
            "\n",
            "Correct response:\n",
            ">> The car is as fast as lightning.\n",
            "\n",
            "Model response:\n",
            ">> The car is as fast as a cheetah.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What type of cloud is typically associated with thunderstorms?\n",
            "\n",
            "Correct response:\n",
            ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
            "\n",
            "Model response:\n",
            ">> The type of cloud typically associated with thunderstorms is a cumulus.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Name the author of 'Pride and Prejudice'.\n",
            "\n",
            "Correct response:\n",
            ">> Jane Austen.\n",
            "\n",
            "Model response:\n",
            ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the periodic symbol for chlorine?\n",
            "\n",
            "Correct response:\n",
            ">> The periodic symbol for chlorine is Cl.\n",
            "\n",
            "Model response:\n",
            ">> The periodic symbol for chlorine is CH3.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Correct the punctuation in the sentence.\n",
            "\n",
            "### Input:\n",
            "Its time to go home.\n",
            "\n",
            "Correct response:\n",
            ">> The corrected sentence should be: 'It's time to go home.'\n",
            "\n",
            "Model response:\n",
            ">> The corrected sentence should be: 'Its time to go home.'\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Rewrite the sentence.\n",
            "\n",
            "### Input:\n",
            "The lecture was delivered in a clear manner.\n",
            "\n",
            "Correct response:\n",
            ">> The lecture was delivered clearly.\n",
            "\n",
            "Model response:\n",
            ">> The lecture was delivered in a clear manner.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a humorous anecdote.\n",
            "\n",
            "Correct response:\n",
            ">> Why was the math book sad? Because it had too many problems!\n",
            "\n",
            "Model response:\n",
            ">> The most popular type of anecdote is a riddle.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Identify the correct spelling: 'recieve' or 'receive'.\n",
            "\n",
            "Correct response:\n",
            ">> The correct spelling is 'receive'.\n",
            "\n",
            "Model response:\n",
            ">> The correct spelling is 'receive'.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Create a sentence using the word 'nostalgia'.\n",
            "\n",
            "Correct response:\n",
            ">> Nostalgia washed over her as she looked through the old photos.\n",
            "\n",
            "Model response:\n",
            ">> The nostalgia of the old games is very strong.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Classify the following numbers as prime or composite.\n",
            "\n",
            "### Input:\n",
            ": 11, 14, 19.\n",
            "\n",
            "Correct response:\n",
            ">> Prime numbers: 11, 19\n",
            "Composite numbers: 14\n",
            "\n",
            "Model response:\n",
            ">> Prime numbers: 15, 36, 49.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the capital of Denmark?\n",
            "\n",
            "Correct response:\n",
            ">> The capital of Denmark is Copenhagen.\n",
            "\n",
            "Model response:\n",
            ">> The capital of Denmark is Copenhagen.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the opposite of 'wet'?\n",
            "\n",
            "Correct response:\n",
            ">> The opposite of 'wet' is 'dry'.\n",
            "\n",
            "Model response:\n",
            ">> The opposite of 'wet' is 'dry'.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Identify the type of sentence.\n",
            "\n",
            "### Input:\n",
            "Did you finish the report?\n",
            "\n",
            "Correct response:\n",
            ">> The type of sentence is interrogative.\n",
            "\n",
            "Model response:\n",
            ">> The type of sentence is imperative.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the chemical symbol for mercury?\n",
            "\n",
            "Correct response:\n",
            ">> Hg.\n",
            "\n",
            "Model response:\n",
            ">> The chemical symbol for mercury is Hg.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Describe the process of active transport in cells.\n",
            "\n",
            "Correct response:\n",
            ">> Active transport is the movement of molecules across a cell membrane from a region of lower concentration to a region of higher concentration, using energy in the form of ATP. This process is crucial for maintaining cellular homeostasis.\n",
            "\n",
            "Model response:\n",
            ">> Transport in cells is the movement of substances from one place to another, called diffusion. It occurs in liquids and gases due to the random movement of particles.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Classify the following items as either solid, liquid, or gas.\n",
            "\n",
            "### Input:\n",
            "Mercury, oxygen, wood\n",
            "\n",
            "Correct response:\n",
            ">> Mercury - Liquid\n",
            "Oxygen - Gas\n",
            "Wood - Solid\n",
            "\n",
            "Model response:\n",
            ">> Solid: Mercury\n",
            "Liquor: Wood\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert 3 kilometers to meters.\n",
            "\n",
            "Correct response:\n",
            ">> 3 kilometers is 3000 meters.\n",
            "\n",
            "Model response:\n",
            ">> 3 kilometers is 2000 meters.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert the sentence to use an indefinite pronoun.\n",
            "\n",
            "### Input:\n",
            "Someone left a note.\n",
            "\n",
            "Correct response:\n",
            ">> A note was left by someone.\n",
            "\n",
            "Model response:\n",
            ">> Someone left a note.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Provide a synonym for 'excited'.\n",
            "\n",
            "Correct response:\n",
            ">> A synonym for 'excited' is 'thrilled'.\n",
            "\n",
            "Model response:\n",
            ">> A synonym for 'excited' is 'furious'.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a sentence that follows the pattern: \"Never have I ever _____ without _____\"\n",
            "\n",
            "Correct response:\n",
            ">> Never have I ever traveled without a map.\n",
            "\n",
            "Model response:\n",
            ">> Never have I ever had without having had.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Pick out the adjective from the following list.\n",
            "\n",
            "### Input:\n",
            "run, tall, quickly\n",
            "\n",
            "Correct response:\n",
            ">> The correct adjective from the list is 'tall.'\n",
            "\n",
            "Model response:\n",
            ">> The adjectives from the list are \"quick\" and \"run.\"\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert 1000 grams to kilograms.\n",
            "\n",
            "Correct response:\n",
            ">> 1000 grams is equal to 1 kilogram.\n",
            "\n",
            "Model response:\n",
            ">> 1000 grams is 0.1 kilograms.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the opposite of 'deep'?\n",
            "\n",
            "Correct response:\n",
            ">> The opposite of 'deep' is 'shallow'.\n",
            "\n",
            "Model response:\n",
            ">> The opposite of 'deep' is 'light'.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Categorize the given list of animals.\n",
            "\n",
            "### Input:\n",
            "Shark, Dolphin, Trout\n",
            "\n",
            "Correct response:\n",
            ">> Fish: Shark, Trout\n",
            "Mammals: Dolphin\n",
            "\n",
            "Model response:\n",
            ">> Dolphin, Shark, Trout\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Translate 'library' into Spanish.\n",
            "\n",
            "Correct response:\n",
            ">> The Spanish word for 'library' is 'biblioteca'.\n",
            "\n",
            "Model response:\n",
            ">> The Spanish word for 'library' is 'libraryas'.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Define the term 'hyperbole'.\n",
            "\n",
            "Correct response:\n",
            ">> Hyperbole is a figure of speech that involves an exaggeration of ideas for the sake of emphasis.\n",
            "\n",
            "Model response:\n",
            ">> Bold is hyperbole, underlined is exaggeration.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Translate 'Hello' into Russian.\n",
            "\n",
            "Correct response:\n",
            ">> Привет (Privet)\n",
            "\n",
            "Model response:\n",
            ">> The Russian translation of 'Hello' is 'Русский' (Dmitry).\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Define the term 'kinetic energy'.\n",
            "\n",
            "Correct response:\n",
            ">> Kinetic energy is the energy that an object possesses due to its motion.\n",
            "\n",
            "Model response:\n",
            ">> Kinetic energy is the amount of force that can be transferred between two objects by the force of gravity acting on a body of equal mass moving at a velocity v.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the opposite of 'hot'?\n",
            "\n",
            "Correct response:\n",
            ">> The opposite of 'hot' is 'cold'.\n",
            "\n",
            "Model response:\n",
            ">> The opposite of 'hot' is 'cold'.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert 5 miles to kilometers.\n",
            "\n",
            "Correct response:\n",
            ">> 5 miles is approximately 8.05 kilometers.\n",
            "\n",
            "Model response:\n",
            ">> 5 miles is 5000 meters.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the chemical formula for magnesium sulfate?\n",
            "\n",
            "Correct response:\n",
            ">> The chemical formula for magnesium sulfate is MgSO4.\n",
            "\n",
            "Model response:\n",
            ">> The chemical formula for magnesium sulfate is MgSO4.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Rewrite the following sentence to replace any cliché phrases.\n",
            "\n",
            "### Input:\n",
            "It's a piece of cake\n",
            "\n",
            "Correct response:\n",
            ">> It's very easy.\n",
            "\n",
            "Model response:\n",
            ">> being a rock star.\n",
            "\n",
            "\n",
            "It's a piece of cake being a rock star.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "List five different types of vegetables.\n",
            "\n",
            "Correct response:\n",
            ">> 1. Carrot\n",
            "2. Broccoli\n",
            "3. Cucumber\n",
            "4. Tomato\n",
            "5. Spinach\n",
            "\n",
            "Model response:\n",
            ">> 1. Carrot\n",
            "2. Spinach\n",
            "3. Broccoli\n",
            "4. Cucumber\n",
            "5. Spinach, Broccoli, and Cucumber\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert 7 kilometers to meters.\n",
            "\n",
            "Correct response:\n",
            ">> 7 kilometers is 7000 meters.\n",
            "\n",
            "Model response:\n",
            ">> 7 kilometers is 9000 meters.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the opposite of 'heavy'?\n",
            "\n",
            "Correct response:\n",
            ">> The opposite of 'heavy' is 'light'.\n",
            "\n",
            "Model response:\n",
            ">> The opposite of 'heavy' is 'light'.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the past tense of 'sing'?\n",
            "\n",
            "Correct response:\n",
            ">> The past tense of 'sing' is 'sang.'\n",
            "\n",
            "Model response:\n",
            ">> The past tense of 'sing' is 'sing.'\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the molecular formula for carbon dioxide?\n",
            "\n",
            "Correct response:\n",
            ">> The molecular formula for carbon dioxide is CO2.\n",
            "\n",
            "Model response:\n",
            ">> The molecular formula for carbon dioxide is CO2.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert this sentence to passive voice\n",
            "\n",
            "### Input:\n",
            "The gardener watered the plants.\n",
            "\n",
            "Correct response:\n",
            ">> The plants were watered by the gardener.\n",
            "\n",
            "Model response:\n",
            ">> The plants were watered by the gardener.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the past tense of 'throw'?\n",
            "\n",
            "Correct response:\n",
            ">> The past tense of 'throw' is 'threw'.\n",
            "\n",
            "Model response:\n",
            ">> The past tense of 'throw' is 'threw'.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Explain what a sonnet is.\n",
            "\n",
            "Correct response:\n",
            ">> A sonnet is a 14-line poem with a specific rhyme scheme and meter, often written in iambic pentameter.\n",
            "\n",
            "Model response:\n",
            ">> A sonnet is a form of poetry that begins with the words 'This is the beginning' and is typically accompanied by a motet or other figure of speech.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a sentence using the word 'innovative'.\n",
            "\n",
            "Correct response:\n",
            ">> The company is known for its innovative products.\n",
            "\n",
            "Model response:\n",
            ">> The chef used innovative ingredients to create a delicious meal.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Provide the plural form of 'cactus'.\n",
            "\n",
            "Correct response:\n",
            ">> The plural form of 'cactus' is 'cacti'.\n",
            "\n",
            "Model response:\n",
            ">> The plural form of 'cactus' is 'cacti'.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Translate the phrase 'Where is the bathroom?' into German.\n",
            "\n",
            "Correct response:\n",
            ">> The German translation of 'Where is the bathroom?' is 'Wo ist die Toilette?'\n",
            "\n",
            "Model response:\n",
            ">> The German translation of 'Where is the bathroom?' is 'Wie alt bist du?'.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a past-tense verb that describes a person laughing.\n",
            "\n",
            "Correct response:\n",
            ">> Laughed.\n",
            "\n",
            "Model response:\n",
            ">> The past-tense verb \"laugh\" is \"laughed.\"\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a sentence using the word 'transient.'\n",
            "\n",
            "Correct response:\n",
            ">> The transient nature of her visit left a lasting impression.\n",
            "\n",
            "Model response:\n",
            ">> The quick brown fox jumps over the lazy dog.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a sentence using the word 'optimistic'.\n",
            "\n",
            "Correct response:\n",
            ">> He remained optimistic despite the challenges he faced.\n",
            "\n",
            "Model response:\n",
            ">> She is optimistic about the future.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Re-word this sentence using an indirect question.\n",
            "\n",
            "### Input:\n",
            "What time is the meeting?\n",
            "\n",
            "Correct response:\n",
            ">> Could you tell me what time the meeting is?\n",
            "\n",
            "Model response:\n",
            ">> The meeting is at the end of the hour.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Categorize the following sentence as a statement, a question, or an exclamation.\n",
            "\n",
            "### Input:\n",
            "What a beautiful day!\n",
            "\n",
            "Correct response:\n",
            ">> Exclamation.\n",
            "\n",
            "Model response:\n",
            ">> What a beautiful day!\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the opposite of 'rich'?\n",
            "\n",
            "Correct response:\n",
            ">> The opposite of 'rich' is 'poor'.\n",
            "\n",
            "Model response:\n",
            ">> The opposite of 'rich' is 'poor'.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Find a synonym for the given verb.\n",
            "\n",
            "### Input:\n",
            "Begin\n",
            "\n",
            "Correct response:\n",
            ">> Commence\n",
            "\n",
            "Model response:\n",
            ">> Start\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Edit the given text to ensure all plural nouns are spelled correctly.\n",
            "\n",
            "### Input:\n",
            "The birds sings beautiful songs.\n",
            "\n",
            "Correct response:\n",
            ">> The birds sing beautiful songs.\n",
            "\n",
            "Model response:\n",
            ">> The birds are singing beautiful songs.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Transform the following sentence into a question using \"could.\"\n",
            "\n",
            "### Input:\n",
            "You can help me tomorrow.\n",
            "\n",
            "Correct response:\n",
            ">> Could you help me tomorrow?\n",
            "\n",
            "Model response:\n",
            ">> Could you help me tomorrow?\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Classify the following items: bicycle, rose, tiger.\n",
            "\n",
            "Correct response:\n",
            ">> Vehicles: Bicycle\n",
            "Plants: Rose\n",
            "Animals: Tiger\n",
            "\n",
            "Model response:\n",
            ">> Animals: Tiger\n",
            "Plants: Rose\n",
            "Electronics: Bicycle\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Define the term 'irony'.\n",
            "\n",
            "Correct response:\n",
            ">> Irony is a figure of speech in which words are used in such a way that their intended meaning is different from the actual meaning of the words.\n",
            "\n",
            "Model response:\n",
            ">> The term 'irony' is a synonym for 'attention'.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Translate 'Welcome' into German.\n",
            "\n",
            "Correct response:\n",
            ">> The German translation of 'Welcome' is 'Willkommen'.\n",
            "\n",
            "Model response:\n",
            ">> The German translation of 'Welcome' is 'Wie alt bist du?'.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Explain the primary function of the human heart.\n",
            "\n",
            "Correct response:\n",
            ">> The primary function of the human heart is to pump blood throughout the body, delivering oxygen and nutrients to tissues and removing carbon dioxide and other wastes.\n",
            "\n",
            "Model response:\n",
            ">> The primary function of the human heart is to pump blood from the lungs to the brain and back. It also includes pumping blood from the brain to the muscles and blood cells in the muscles.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Reword the following sentence to the future tense.\n",
            "\n",
            "### Input:\n",
            "He is reading a novel inspired by his grandmother.\n",
            "\n",
            "Correct response:\n",
            ">> He will be reading a novel inspired by his grandmother.\n",
            "\n",
            "Model response:\n",
            ">> He is reading a novel inspired by his grandmother.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert the given sentence into active voice.\n",
            "\n",
            "### Input:\n",
            "The law was passed by the government.\n",
            "\n",
            "Correct response:\n",
            ">> The government passed the law.\n",
            "\n",
            "Model response:\n",
            ">> The law was passed by the government.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Create a sentence using the word 'inevitable'.\n",
            "\n",
            "Correct response:\n",
            ">> The confrontation was inevitable given the circumstances.\n",
            "\n",
            "Model response:\n",
            ">> The collapse of the company was inevitable due to poor management.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Categorize the following sentence as either factual or opinion-based.\n",
            "\n",
            "### Input:\n",
            "Chocolate is the best dessert.\n",
            "\n",
            "Correct response:\n",
            ">> Opinion-based.\n",
            "\n",
            "Model response:\n",
            ">> The statement is based on fact that chocolate is the best dessert.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is an antonym of 'old'?\n",
            "\n",
            "Correct response:\n",
            ">> young.\n",
            "\n",
            "Model response:\n",
            ">> An antonym of 'old' is 'young'.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Provide a synonym for 'hardworking'.\n",
            "\n",
            "Correct response:\n",
            ">> A synonym for 'hardworking' is 'diligent'.\n",
            "\n",
            "Model response:\n",
            ">> A synonym for 'hardworking' is 'smart'.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the boiling point of sulfur in Celsius?\n",
            "\n",
            "Correct response:\n",
            ">> The boiling point of sulfur is 444.6 degrees Celsius.\n",
            "\n",
            "Model response:\n",
            ">> The boiling point of sulfur is -114.5 degrees Celsius.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the plural form of 'child'?\n",
            "\n",
            "Correct response:\n",
            ">> The plural form of 'child' is 'children'.\n",
            "\n",
            "Model response:\n",
            ">> The plural form of 'child' is 'children'.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is an antonym of 'complicated'?\n",
            "\n",
            "Correct response:\n",
            ">> An antonym of 'complicated' is 'simple'.\n",
            "\n",
            "Model response:\n",
            ">> An antonym of 'complicated' is 'simplified'.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Name three forms of water.\n",
            "\n",
            "Correct response:\n",
            ">> The three forms of water are solid (ice), liquid (water), and gas (steam).\n",
            "\n",
            "Model response:\n",
            ">> Three forms of water are rain, sleet, and snow.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Rewrite this sentence as a question.\n",
            "\n",
            "### Input:\n",
            "The dog chased the cat.\n",
            "\n",
            "Correct response:\n",
            ">> Did the dog chase the cat?\n",
            "\n",
            "Model response:\n",
            ">> The question should be, \"What is the primary function of the digestive system in humans?\"\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Split the following sentence into two declarative sentences: 'The movie was long but interesting.'\n",
            "\n",
            "Correct response:\n",
            ">> The movie was long. It was interesting.\n",
            "\n",
            "Model response:\n",
            ">> and 'The movie was boring but interesting.'\n",
            "\n",
            "### Input:\n",
            "The movie was long but interesting.\n",
            "\n",
            "\n",
            "The movie was boring but interesting.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Classify the following substances as acid, base, or neutral.\n",
            "\n",
            "### Input:\n",
            "Lemon juice, Soap, Water\n",
            "\n",
            "Correct response:\n",
            ">> Acid: Lemon juice\n",
            "Base: Soap\n",
            "Neutral: Water\n",
            "\n",
            "Model response:\n",
            ">> Acid: Soap\n",
            "Base: Lemon juice\n",
            "Neutral: Soap\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is a synonym of 'sad'?\n",
            "\n",
            "Correct response:\n",
            ">> A synonym for 'sad' is 'unhappy'.\n",
            "\n",
            "Model response:\n",
            ">> A synonym for 'sad' is 'angry'.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Correct any spelling mistakes in the given sentence.\n",
            "\n",
            "### Input:\n",
            "I prefer homemade cookies to store boaght.\n",
            "\n",
            "Correct response:\n",
            ">> I prefer homemade cookies to store bought.\n",
            "\n",
            "Model response:\n",
            ">> I prefer homemade cookies to store boaght.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a sentence using the word 'transient'.\n",
            "\n",
            "Correct response:\n",
            ">> His stay in the city was transient, lasting only a couple of days.\n",
            "\n",
            "Model response:\n",
            ">> The quick brown fox jumps over the lazy dog.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Translate 'I am lost' into Italian.\n",
            "\n",
            "Correct response:\n",
            ">> The Italian translation of 'I am lost' is 'Mi sono perso' (if male) or 'Mi sono persa' (if female).\n",
            "\n",
            "Model response:\n",
            ">> Il più eso?\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Classify this text as a technical document or a narrative.\n",
            "\n",
            "### Input:\n",
            "This manual provides instructions for installing the software.\n",
            "\n",
            "Correct response:\n",
            ">> Technical document\n",
            "\n",
            "Model response:\n",
            ">> This document is a narrative.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Sort the following list in descending order.\n",
            "\n",
            "### Input:\n",
            "10, 2, 25, 16, 7\n",
            "\n",
            "Correct response:\n",
            ">> 25, 16, 10, 7, 2.\n",
            "\n",
            "Model response:\n",
            ">> The list of the first 25 items is: 2, 15, 7, 25.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Translate 'Can I have some water?' into French.\n",
            "\n",
            "Correct response:\n",
            ">> Puis-je avoir de l'eau?\n",
            "\n",
            "Model response:\n",
            ">> Can I have some water?\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Create a simile with the word 'as cold as'.\n",
            "\n",
            "Correct response:\n",
            ">> Her hands were as cold as ice.\n",
            "\n",
            "Model response:\n",
            ">> The room was as cold as ice.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Classify the following words by their grammatical categories: swim, beautiful, quickly\n",
            "\n",
            "Correct response:\n",
            ">> Swim: Verb\n",
            "Beautiful: Adjective\n",
            "Quickly: Adverb\n",
            "\n",
            "Model response:\n",
            ">> The words \"swim\" and \"beautiful\" are synonyms.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Calculate the density of an object with a mass of 15 grams and a volume of 5 cubic centimeters.\n",
            "\n",
            "Correct response:\n",
            ">> The density of the object is 3 grams per cubic centimeter.\n",
            "\n",
            "Model response:\n",
            ">> The density of the object is 15 grams per cubic centimeter.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the abbreviation for 'Master of Business Administration'?\n",
            "\n",
            "Correct response:\n",
            ">> The abbreviation for 'Master of Business Administration' is MBA.\n",
            "\n",
            "Model response:\n",
            ">> The abbreviation for 'Master of Business Administration' is MBA.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert the following number from Roman numerals: IX.\n",
            "\n",
            "Correct response:\n",
            ">> The number IX in Roman numerals is 9.\n",
            "\n",
            "Model response:\n",
            ">> 5 to decimal: 255.\n",
            "\n",
            "\n",
            "The decimal equivalent of the Roman numerals IX.5 is 255.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the opposite of 'horizontal'?\n",
            "\n",
            "Correct response:\n",
            ">> The opposite of 'horizontal' is 'vertical'.\n",
            "\n",
            "Model response:\n",
            ">> The opposite of 'horizontal' is 'vertical'.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Translate 'Where can I buy tickets?' into Italian.\n",
            "\n",
            "Correct response:\n",
            ">> The Italian translation for 'Where can I buy tickets?' is 'Dove posso comprare i biglietti?'\n",
            "\n",
            "Model response:\n",
            ">> Dove è la prego?\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Rewrite the following sentence to replace any clichés.\n",
            "\n",
            "### Input:\n",
            "He was as cool as a cucumber.\n",
            "\n",
            "Correct response:\n",
            ">> He remained very calm.\n",
            "\n",
            "Model response:\n",
            ">> He was as cool as a cucumber.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Identify the main verb in the sentence.\n",
            "\n",
            "### Input:\n",
            "The dog barked loudly.\n",
            "\n",
            "Correct response:\n",
            ">> The main verb in the sentence is 'barked'.\n",
            "\n",
            "Model response:\n",
            ">> The main verb in the sentence is 'bark.'\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a sentence using the word 'elucidate'.\n",
            "\n",
            "Correct response:\n",
            ">> The professor attempted to elucidate the complex topic for his students.\n",
            "\n",
            "Model response:\n",
            ">> The teacher was able to quickly solve the problem.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Correct the sentence.\n",
            "\n",
            "### Input:\n",
            "Me and my friend went to the store.\n",
            "\n",
            "Correct response:\n",
            ">> My friend and I went to the store.\n",
            "\n",
            "Model response:\n",
            ">> We went to the store.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the formula for calculating work done?\n",
            "\n",
            "Correct response:\n",
            ">> The formula for calculating work done is work = force × distance.\n",
            "\n",
            "Model response:\n",
            ">> The formula for calculating work done is total weekly output minus weekly input.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the chemical formula for ammonium nitrate?\n",
            "\n",
            "Correct response:\n",
            ">> The chemical formula for ammonium nitrate is NH4NO3.\n",
            "\n",
            "Model response:\n",
            ">> The chemical formula for ammonium nitrate is NH3.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the molecular formula for water?\n",
            "\n",
            "Correct response:\n",
            ">> The molecular formula for water is H2O.\n",
            "\n",
            "Model response:\n",
            ">> The molecular formula for water (dihydrogen monoxide) is H2O.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Rewrite the given sentence to describe the same thing in a positive way.\n",
            "\n",
            "### Input:\n",
            "The food was not good.\n",
            "\n",
            "Correct response:\n",
            ">> The food could use some improvement.\n",
            "\n",
            "Model response:\n",
            ">> The food was not good.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the opposite of 'lazy'?\n",
            "\n",
            "Correct response:\n",
            ">> The opposite of 'lazy' is 'diligent'.\n",
            "\n",
            "Model response:\n",
            ">> The opposite of 'lazy' is 'busy'.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Name three essential vitamins for human health.\n",
            "\n",
            "Correct response:\n",
            ">> 1. Vitamin A\n",
            "2. Vitamin C\n",
            "3. Vitamin D\n",
            "\n",
            "Model response:\n",
            ">> 1. Calcium\n",
            "2. Iron\n",
            "3. Magnesium\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Define the term 'simile'.\n",
            "\n",
            "Correct response:\n",
            ">> A simile is a figure of speech that directly compares two different things, often introduced with the words 'like' or 'as'.\n",
            "\n",
            "Model response:\n",
            ">> A simile is a figure of speech in which a person or thing is given a simile or metaphor to describe a person, thing, or action.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the boiling point of chlorine in Celsius?\n",
            "\n",
            "Correct response:\n",
            ">> The boiling point of chlorine is -34 degrees Celsius.\n",
            "\n",
            "Model response:\n",
            ">> The boiling point of chlorine is -183 degrees Celsius.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Translate 'My name is' into French.\n",
            "\n",
            "Correct response:\n",
            ">> The French translation of 'My name is' is 'Je m'appelle'.\n",
            "\n",
            "Model response:\n",
            ">> The French word for 'My name is' is 'D'où viens-tu?'.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert 200 centimeters to meters.\n",
            "\n",
            "Correct response:\n",
            ">> 200 centimeters is 2 meters.\n",
            "\n",
            "Model response:\n",
            ">> 200 centimeters is 0.2 meters.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the chemical symbol for zinc?\n",
            "\n",
            "Correct response:\n",
            ">> Zn.\n",
            "\n",
            "Model response:\n",
            ">> The chemical symbol for zinc is Z.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the formula for calculating force?\n",
            "\n",
            "Correct response:\n",
            ">> The formula for calculating force is Force = mass x acceleration.\n",
            "\n",
            "Model response:\n",
            ">> The formula for calculating force is the product of the two forces, the force that is applied and the force that is resisted.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a sentence using the word 'innovative'.\n",
            "\n",
            "Correct response:\n",
            ">> The company's innovative approach set it apart from its competitors.\n",
            "\n",
            "Model response:\n",
            ">> The chef used innovative ingredients to create a delicious meal.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a sentence using the word 'generous'.\n",
            "\n",
            "Correct response:\n",
            ">> He is very generous and always helps those in need.\n",
            "\n",
            "Model response:\n",
            ">> He is generous and always helps others in need.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is an antonym of 'sharp'?\n",
            "\n",
            "Correct response:\n",
            ">> An antonym of 'sharp' is 'dull'.\n",
            "\n",
            "Model response:\n",
            ">> An antonym of 'sharp' is 'thin'.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Describe the basic structure of a neuron.\n",
            "\n",
            "Correct response:\n",
            ">> A neuron consists of three main parts: the cell body, which contains the nucleus; dendrites, which receive signals from other neurons; and an axon, which transmits signals to other neurons, muscles, or glands.\n",
            "\n",
            "Model response:\n",
            ">> A neuron is a type of cell that receives signals from other neurons in the same cell.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Translate 'see you later' into Spanish.\n",
            "\n",
            "Correct response:\n",
            ">> Hasta luego\n",
            "\n",
            "Model response:\n",
            ">> See you later.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What are the first 10 elements of the Fibonacci sequence?\n",
            "\n",
            "Correct response:\n",
            ">> 0, 1, 1, 2, 3, 5, 8, 13, 21, 34.\n",
            "\n",
            "Model response:\n",
            ">> 1, 3, 5, 7, 9, 11, 13, 15, 17, 19.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is an antonym of 'transparent'?\n",
            "\n",
            "Correct response:\n",
            ">> An antonym of 'transparent' is 'opaque'.\n",
            "\n",
            "Model response:\n",
            ">> An antonym of 'transparent' is 'illuminate'.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the past tense of 'think'?\n",
            "\n",
            "Correct response:\n",
            ">> The past tense of 'think' is 'thought'.\n",
            "\n",
            "Model response:\n",
            ">> The past tense of 'think' is 'thought.'\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Classify each sentence as either declarative, interrogative, imperative, or exclamatory.\n",
            "\n",
            "### Input:\n",
            "Please open the door.\n",
            "\n",
            "Correct response:\n",
            ">> The classification of the sentence 'Please open the door.' is imperative.\n",
            "\n",
            "Model response:\n",
            ">> The sentence \"Please open the door\" is interrogative.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Rewrite the sentence to use a negative adverb.\n",
            "\n",
            "### Input:\n",
            "She always remembers to call.\n",
            "\n",
            "Correct response:\n",
            ">> She never forgets to call.\n",
            "\n",
            "Model response:\n",
            ">> She always remembers to call.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert 50 miles per hour to kilometers per hour.\n",
            "\n",
            "Correct response:\n",
            ">> 50 miles per hour is approximately 80.47 kilometers per hour.\n",
            "\n",
            "Model response:\n",
            ">> 50 miles per hour is approximately 32.64 kilometers per hour.\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we have everything trained on, the result is in the hand. Think wisely, the model looks quite OK now in our eyes but how OK? What will be the quantitive result?\n",
        "\n",
        "- There are many popular evaluation method over out there, I am opening this research into your hand.\n",
        "- I am going to evaluate through Llama 2 which is a product of ollama.\n",
        "\n",
        "As we are going to run ollama locally, and evaluate, I have to shift this notebook to the VSCode jupyter notebook.\n",
        "\n",
        "How did I transfer it? Simply saving and loading the model there, and running afterwards, no redundant training out there."
      ],
      "metadata": {
        "id": "oYOFO_gJrUYL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "G16Whg2rrUpr",
        "outputId": "6b307a73-f3c4-4b1c-d750-94227faad988"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ollama running: True\n"
          ]
        }
      ],
      "source": [
        "import psutil\n",
        "\n",
        "def check_if_running(process_name):\n",
        "    running = False\n",
        "    for proc in psutil.process_iter([\"name\"]):\n",
        "        if process_name in proc.info[\"name\"]:\n",
        "            running = True\n",
        "            break\n",
        "    return running\n",
        "\n",
        "ollama_running = check_if_running(\"ollama\")\n",
        "\n",
        "if not ollama_running:\n",
        "    raise RuntimeError(\"Ollama not running. Launch ollama before proceeding.\")\n",
        "print(\"Ollama running:\", check_if_running(\"ollama\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54GVBotgrH6A",
        "outputId": "113ce4ae-ac88-488f-b624-82c398922539"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ollama running: True\n"
          ]
        }
      ],
      "source": [
        "import psutil\n",
        "\n",
        "def check_if_running(process_name):\n",
        "    running = False\n",
        "    for proc in psutil.process_iter([\"name\"]):\n",
        "        if process_name in proc.info[\"name\"]:\n",
        "            running = True\n",
        "            break\n",
        "    return running\n",
        "\n",
        "ollama_running = check_if_running(\"ollama\")\n",
        "\n",
        "if not ollama_running:\n",
        "    raise RuntimeError(\"Ollama not running. Launch ollama before proceeding.\")\n",
        "print(\"Ollama running:\", check_if_running(\"ollama\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w558pToxrH6B"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "An alternative to the ollama run command for interacting with the model is through its\n",
        "REST API using Python.\n",
        "\n",
        "The following query_model function demonstrates how to use the\n",
        "API:</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaT6uIzyrH6B"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "\n",
        "Step 1: Create the data payload as a dictionary\n",
        "    \n",
        "Step 2: Convert the dictionary to a JSON formatted string and encode it to bytes\n",
        "    \n",
        "Step 3: Create a request object, setting the method to POST and adding necessary headers\n",
        "    \n",
        "Step 4: Send the request and capture the response\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjZy389ArH6B"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "\n",
        "def query_model(\n",
        "    prompt,\n",
        "    model=\"llama3\",\n",
        "    url=\"http://localhost:11434/api/chat\"\n",
        "):\n",
        "    # Create the data payload as a dictionary\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        \"options\": {     # Settings below are required for deterministic responses\n",
        "            \"seed\": 123,\n",
        "            \"temperature\": 0,\n",
        "            \"num_ctx\": 2048\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    # Convert the dictionary to a JSON formatted string and encode it to bytes\n",
        "    payload = json.dumps(data).encode(\"utf-8\")\n",
        "\n",
        "    # Create a request object, setting the method to POST and adding necessary headers\n",
        "    request = urllib.request.Request(\n",
        "        url,\n",
        "        data=payload,\n",
        "        method=\"POST\"\n",
        "    )\n",
        "    request.add_header(\"Content-Type\", \"application/json\")\n",
        "\n",
        "    # Send the request and capture the response\n",
        "    response_data = \"\"\n",
        "    with urllib.request.urlopen(request) as response:\n",
        "        # Read and decode the response\n",
        "        while True:\n",
        "            line = response.readline().decode(\"utf-8\")\n",
        "            if not line:\n",
        "                break\n",
        "            response_json = json.loads(line)\n",
        "            response_data += response_json[\"message\"][\"content\"]\n",
        "\n",
        "    return response_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rypamJuWrH6C"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Before running the subsequent code cells in this notebook, ensure that Ollama is still\n",
        "running. The previous code cells should print \"Ollama running: True\" to confirm that the\n",
        "model is active and ready to receive requests.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnvoRDwVrH6C"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Here's an example of how to use the query_llama function we just implemented:\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LliHBBkdrH6C",
        "outputId": "8880064c-df8f-4f3d-f641-4db0caeb0674"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Llamas are herbivores, which means they primarily feed on plant-based foods. Their diet typically consists of:\n",
            "\n",
            "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and even weeds.\n",
            "2. Hay: High-quality hay, such as alfalfa or timothy hay, is a staple in a llama's diet. They enjoy the sweet taste and texture of fresh hay.\n",
            "3. Grains: Llamas may receive grains like oats, barley, or corn as part of their daily ration. However, it's essential to provide these grains in moderation, as they can be high in calories.\n",
            "4. Fruits and vegetables: Llamas enjoy a variety of fruits and veggies, such as apples, carrots, sweet potatoes, and leafy greens like kale or spinach.\n",
            "5. Minerals: Llamas require access to mineral supplements, which help maintain their overall health and strong bones.\n",
            "\n",
            "In the wild, llamas might also eat:\n",
            "\n",
            "1. Leaves: They'll munch on leaves from trees and shrubs, like willow or cedar.\n",
            "2. Bark: In some cases, llamas may eat the bark of certain trees, like aspen or birch.\n",
            "3. Mosses: Llamas have been known to graze on mosses and other types of non-woody plants.\n",
            "\n",
            "In captivity, llama owners typically provide a balanced diet that includes a mix of hay, grains, and fruits/vegetables. It's essential to consult with a veterinarian or experienced llama breeder to determine the best feeding plan for your llama.\n"
          ]
        }
      ],
      "source": [
        "model = \"llama3\"\n",
        "result = query_model(\"What do Llamas eat?\", model)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY7z3LdPrH6D"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "Using the query_model function defined earlier, we can evaluate the responses generated\n",
        "by our finetuned model with a prompt that prompts the Llama 3 model to rate our\n",
        "finetuned model's responses on a scale from 0 to 100 based on the given test set response\n",
        "as reference.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeZSeRKzrH6E"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "\n",
        "The following generate_model_scores function uses a modified the prompt telling the\n",
        "model to \"Respond with the integer number only.\":\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFdKXF5JrH6E",
        "outputId": "e4d54d3b-b43d-4244-e6fa-bdca624e8518"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset response:\n",
            ">> The car is as fast as lightning.\n",
            "\n",
            "Model response:\n",
            ">> The car is as fast as a cheetah.\n",
            "\n",
            "Score:\n",
            ">> 85\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
            "\n",
            "Model response:\n",
            ">> The type of cloud typically associated with thunderstorms is a cumulus.\n",
            "\n",
            "Score:\n",
            ">> 60\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> Jane Austen.\n",
            "\n",
            "Model response:\n",
            ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
            "\n",
            "Score:\n",
            ">> 98\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> The periodic symbol for chlorine is Cl.\n",
            "\n",
            "Model response:\n",
            ">> The periodic symbol for chlorine is CH3.\n",
            "\n",
            "Score:\n",
            ">> 4\n",
            "\n",
            "-------------------------\n",
            "\n",
            "Dataset response:\n",
            ">> The corrected sentence should be: 'It's time to go home.'\n",
            "\n",
            "Model response:\n",
            ">> The corrected sentence should be: 'Its time to go home.'\n",
            "\n",
            "Score:\n",
            ">> 20\n",
            "\n",
            "-------------------------\n",
            "The average score of the model is:  53\n"
          ]
        }
      ],
      "source": [
        "def avg_score(scores):\n",
        "    total = 0\n",
        "    for score in scores:\n",
        "        total = total + score\n",
        "    return total//len(scores)\n",
        "\n",
        "scores = []\n",
        "for entry in test_data[:5]:\n",
        "    prompt = (\n",
        "            f\"Given the input `{format_input(entry)}` \"\n",
        "            f\"and correct output `{entry['output']}`, \"\n",
        "            f\"score the model response `{entry['model_response']}`\"\n",
        "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
        "            f\"Respond with the integer number only.\"\n",
        "        )\n",
        "    print(\"\\nDataset response:\")\n",
        "    print(\">>\", entry['output'])\n",
        "    print(\"\\nModel response:\")\n",
        "    print(\">>\", entry[\"model_response\"])\n",
        "    print(\"\\nScore:\")\n",
        "    score = int(query_model(prompt, model))\n",
        "    print(\">>\", str(score))\n",
        "    scores.append(score)\n",
        "    print(\"\\n-------------------------\")\n",
        "\n",
        "print(f\"The average score of the model is: \", avg_score(scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bF2YLhBNrH6F"
      },
      "outputs": [],
      "source": [
        "def generate_model_scores(json_data, json_key, model=\"llama3\"):\n",
        "    scores = []\n",
        "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
        "        prompt = (\n",
        "            f\"Given the input `{format_input(entry)}` \"\n",
        "            f\"and correct output `{entry['output']}`, \"\n",
        "            f\"score the model response `{entry[json_key]}`\"\n",
        "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
        "            f\"Respond with the integer number only.\"\n",
        "        )\n",
        "        score = query_model(prompt, model)\n",
        "        try:\n",
        "            scores.append(int(score))\n",
        "        except ValueError:\n",
        "            print(f\"Could not convert score: {score}\")\n",
        "            continue\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hVF1egQrH6F"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "\n",
        "When you run the above code, you will see that the evaluation output shows that our finetuned model achieves an average score above 50,\n",
        "which provides a useful benchmark for comparison against other models or for\n",
        "experimenting with different training configurations to improve the model's performance.\n",
        "\n",
        "It's worth noting that Ollama is not entirely deterministic at the time of this writing,\n",
        "which means that the scores you obtain might slightly vary from the ones presented above.\n",
        "    \n",
        "To obtain more robust results, you can repeat the evaluation multiple times and average\n",
        "the resulting scores.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Created by **Md. Shadikur Rahman Sheam**.\n",
        "- [Linkedin](https://www.linkedin.com/in/md-shadikur-rahman-sheam-3826482b3/)\n",
        "- [Github](https://github.com/sadikurSenpai)\n",
        "- [Facebook](https://www.facebook.com/profile.php?id=100091833665881)"
      ],
      "metadata": {
        "id": "xToCzBiAYtLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Special thanks to Sebastian Raschka for his amazing book."
      ],
      "metadata": {
        "id": "m3qvNTEQY2i0"
      }
    }
  ]
}